{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bb274025",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from utils import *\n",
    "\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5545cdae",
   "metadata": {},
   "source": [
    "## set up dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0989a14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_idx = 4\n",
    "\n",
    "# load raw data\n",
    "precursor_info_path = './dataset/precursor_info.tsv'\n",
    "matrix_path = './dataset/matrix_mz_prob_mean_var.npy'\n",
    "indices_path = f'./dataset/train_test_split_set_{set_idx}.npy'\n",
    "predictions_folder = f'./predictions/{set_idx}_'\n",
    "\n",
    "precursor_df = pd.read_csv(precursor_info_path, sep='\\t')\n",
    "# each row is a precursor, which is a pair of (peptide_sequence, charge)\n",
    "# columns: [\n",
    "#   'precursor_index',          # corresponds to the index of the first order of the matrix\n",
    "#   'sequence',                 # peptide sequence, no modification by now\n",
    "#   'charge',                   # charge of the precursor\n",
    "#   'num_PSMs'                  # number of spectra that associated with this precursor\n",
    "# ]\n",
    "\n",
    "matrix = np.load(matrix_path)\n",
    "# matrix is a 3D numpy array with shape (num_precursors, num_tokens_to_predict, 4)\n",
    "#   num_precursors: the ith precursor corresponds to the ith row in precursor_df\n",
    "#   num_tokens_to_predict: the number of tokens (peak) to predict in the ith precursor\n",
    "#   4: for each token, there are 4 values: [\n",
    "#       \"m/z\",                  # the m/z value of the token (peak)\n",
    "#       \"probability\",          # the probability of the token (peak) to be observed\n",
    "#       \"mean_intensity\",       # the mean intensity of the token (peak)\n",
    "#       \"var_intensity\"         # the variance of the intensity of the token (peak)\n",
    "\n",
    "\n",
    "\n",
    "loaded_data = np.load(indices_path, allow_pickle=True).reshape(1)[0]\n",
    "\n",
    "# loaded_data is a dictionary with the following keys:\n",
    "#   'train_indices': the indices of the training set\n",
    "#   'test_indices': the indices of the test set\n",
    "#   'train_indices' and 'test_indices' are numpy arrays of shape (80% num_samples,) and (20% num_samples,) respectively\n",
    "train_indices = loaded_data['train_indices']\n",
    "test_indices = loaded_data['test_indices']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "84435d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "610116\n",
      "610117\n",
      "610116\n"
     ]
    }
   ],
   "source": [
    "np.concatenate((train_indices, test_indices), axis=0).max()\n",
    "matrix.shape[0]\n",
    "\n",
    "# number of rows\n",
    "print(precursor_df.index.max())\n",
    "print(precursor_df.shape[0])\n",
    "print(precursor_df['precursor_index'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "21350826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create peak mask: probabilities outside mask shouldn't have value and are set to -1\n",
    "\n",
    "peak_mask = np.zeros((matrix.shape[0], matrix.shape[1]), dtype=bool)\n",
    "\n",
    "def get_ion_mask(seq_len, charge, max_seq_len):\n",
    "    # a2+ ions\n",
    "    mask = [True]\n",
    "    if charge > 3:\n",
    "        charge = 3\n",
    "    # b/y ions with charge 1/2/3\n",
    "    for ion in range(1, 3):\n",
    "        for chr in range(1, 4):\n",
    "            if chr > charge:\n",
    "                for seq_idx in range(1, max_seq_len):\n",
    "                    mask.append(False)\n",
    "            else:\n",
    "                for seq_idx in range(1, max_seq_len):\n",
    "                    if seq_idx < seq_len:\n",
    "                        mask.append(True)\n",
    "                    else:\n",
    "                        mask.append(False)\n",
    "    return mask\n",
    "\n",
    "for i in range(matrix.shape[0]):\n",
    "    seq_len = len(precursor_df['sequence'][i])\n",
    "    charge = precursor_df['charge'][i]\n",
    "    max_seq_len = matrix.shape[1]\n",
    "    mask = get_ion_mask(seq_len, charge, 40)\n",
    "    peak_mask[i, :] = mask\n",
    "\n",
    "assert matrix[:, :, 1][~peak_mask].max() < 1e-8\n",
    "\n",
    "\n",
    "probabilities = matrix[:, :, 1].copy()\n",
    "probabilities[~peak_mask] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "66b46262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create filter mask: only precursors satisfying the following conditions are kept\n",
    "min_num_psms = 30\n",
    "max_charge = 100\n",
    "max_seq_length = 100\n",
    "\n",
    "\n",
    "filtered_indices = precursor_df[\n",
    "    (precursor_df['num_PSMs'] >= min_num_psms) &\n",
    "    (precursor_df['charge'] <= max_charge) &\n",
    "    (precursor_df['sequence'].str.len() <= max_seq_length)\n",
    "].index\n",
    "\n",
    "filter_mask = np.zeros((matrix.shape[0],), dtype=bool)\n",
    "filter_mask[filtered_indices] = True\n",
    "\n",
    "# filtered_sequences = precursor_df['sequence'].iloc[filtered_indices].values\n",
    "# filtered_charges = precursor_df['charge'].iloc[filtered_indices].values\n",
    "# filtered_num_PSMs = precursor_df['num_PSMs'].iloc[filtered_indices].values\n",
    "\n",
    "# filtered_probabilities = probabilities[filtered_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "103773f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence: AAEAVAAAVGTGATTAAVMAAAGIAAIGPAKELEAER\n",
      "Charge: 4\n",
      "Encoded Sequence: [ 4 13 13  3 13 12 13 13 13 12 18  4 18 13  4  4 13 13 12  5 13 13 13 18\n",
      " 15 13 13 15 18  1 13 14  3 11  3 13  3 16  0  0  0]\n",
      "Probabilities (first 20): [0.853659 0.       0.853659 0.97561  1.       1.       1.       0.97561\n",
      " 1.       0.951219 0.878049 0.780488 0.634146 0.658537 0.439024 0.170732\n",
      " 0.146341 0.512195 0.097561 0.      ]\n"
     ]
    }
   ],
   "source": [
    "# encode sequences and charges into integers. 0 is reserved for padding\n",
    "sequences = precursor_df['sequence'].values\n",
    "charges = precursor_df['charge'].values\n",
    "num_PSMs = precursor_df['num_PSMs'].values\n",
    "\n",
    "\n",
    "unique_chars = set(''.join(sequences))\n",
    "char_to_int = {char: i+1 for i, char in enumerate(unique_chars)}\n",
    "# reserve 0 for padding\n",
    "\n",
    "unique_charges = set(charges)\n",
    "charge_to_int = {charge: i+1 for i, charge in enumerate(unique_charges)}\n",
    "# reserve 0 for padding\n",
    "\n",
    "max_sequence_length = max(len(seq) for seq in sequences)\n",
    "def encode_sequence_and_charge(seq, charge):\n",
    "    seq_encoded = np.zeros(max_sequence_length+1, dtype=int)\n",
    "    seq_encoded[0] = charge_to_int[charge]\n",
    "    for i, char in enumerate(seq):\n",
    "        seq_encoded[i+1] = char_to_int[char]\n",
    "    return seq_encoded\n",
    "\n",
    "\n",
    "\n",
    "X = np.array([encode_sequence_and_charge(seq, charge) for seq, charge in zip(sequences, charges)])\n",
    "Y = probabilities\n",
    "\n",
    "\n",
    "idx = 1234\n",
    "print(f\"Sequence: {sequences[idx]}\")\n",
    "print(f\"Charge: {charges[idx]}\")\n",
    "print(f\"Encoded Sequence: {X[idx, :]}\")\n",
    "print(f\"Probabilities (first 20): {Y[idx, :20]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4f99b613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot Encoded Sequence: [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0]\n"
     ]
    }
   ],
   "source": [
    "# create one-hot version of X\n",
    "X_1hot = np.zeros((X.shape[0], X.shape[1], len(char_to_int)), dtype=int)\n",
    "for i in range(X.shape[0]):\n",
    "    for j in range(X.shape[1]):\n",
    "        if X[i, j] != 0:\n",
    "            X_1hot[i, j, X[i, j]-1] = 1\n",
    "X_1hot = X_1hot.reshape(X.shape[0], -1)\n",
    "\n",
    "idx = 1234\n",
    "print(f\"One-hot Encoded Sequence: {X_1hot[idx, :]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "69c914ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "\n",
    "train_mask = np.zeros(matrix.shape[0], dtype=bool)\n",
    "train_mask[train_indices] = True\n",
    "train_mask = np.logical_and(train_mask, filter_mask)\n",
    "\n",
    "test_mask = np.zeros(matrix.shape[0], dtype=bool)\n",
    "test_mask[test_indices] = True\n",
    "test_mask = np.logical_and(test_mask, filter_mask)\n",
    "\n",
    "X_train = X_1hot[train_mask]\n",
    "Y_train = Y[train_mask]\n",
    "X_test = X_1hot[test_mask]\n",
    "Y_test = Y[test_mask]\n",
    "\n",
    "# Y_total = Y\n",
    "# X_total = X_1hot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac2a26d",
   "metadata": {},
   "source": [
    "## Value statistics predictor\n",
    "Estimate probability of a peak based on its frequency in the training set.\n",
    "\n",
    "Two seperate estimators are constructed:\n",
    "1. estimator A conditions on ion and charge\n",
    "2. estimator B conditions on ion & charge and pre-(suf-)fix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a011ef87",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_train = sequences[train_mask]\n",
    "sequences_test = sequences[test_mask]\n",
    "charges_train = charges[train_mask]\n",
    "charges_test = charges[test_mask]\n",
    "PSMs_train = num_PSMs[train_mask]\n",
    "PSMs_test = num_PSMs[test_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c3e946bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct estimators\n",
    "\n",
    "# estimator A conditions on ion and charge\n",
    "estimA = {}\n",
    "# estimator A conditions on ion and charge and pre/suffix\n",
    "estimB = {}\n",
    "\n",
    "for idx, seq in enumerate(sequences_train):\n",
    "    num_PSMs = PSMs_train[idx]\n",
    "    for pos in range(Y.shape[1]):\n",
    "\n",
    "        if Y_train[idx, pos] < -0.1:\n",
    "            continue\n",
    "\n",
    "        ion_type, ion_charge, cut_pos = frag_ion_names[pos]\n",
    "        if ion_type == 'a' or ion_type == 'b':\n",
    "            pre_suf_fix = seq[:cut_pos]\n",
    "        else:\n",
    "            pre_suf_fix = seq[-cut_pos:]\n",
    "        keyA = (ion_type, ion_charge)\n",
    "        keyB = (ion_type, ion_charge, pre_suf_fix)\n",
    "        prob = Y_train[idx, pos]\n",
    "\n",
    "        if keyA not in estimA:\n",
    "            estimA[keyA] = (prob*num_PSMs, num_PSMs)\n",
    "        else:\n",
    "            estimA[keyA] = (estimA[keyA][0] + prob*num_PSMs, estimA[keyA][1] + num_PSMs)\n",
    "        \n",
    "        if keyB not in estimB:\n",
    "            estimB[keyB] = (prob*num_PSMs, num_PSMs)\n",
    "        else:\n",
    "            estimB[keyB] = (estimB[keyB][0] + prob*num_PSMs, estimB[keyB][1] + num_PSMs)\n",
    "\n",
    "# calculate mean\n",
    "estimA0 = {x: estimA[x][0]/estimA[x][1] for x in estimA}\n",
    "estimB0 = {x: estimB[x][0]/estimB[x][1] for x in estimB}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b690ff83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate estimators on the whole dataset\n",
    "\n",
    "predA = np.zeros((Y.shape[0], Y.shape[1]))\n",
    "predB = np.zeros((Y.shape[0], Y.shape[1]))\n",
    "\n",
    "for idx, seq in enumerate(sequences):\n",
    "    for pos in range(Y.shape[1]):\n",
    "\n",
    "        if Y[idx, pos] < -0.1:\n",
    "            predA[idx, pos] = -1\n",
    "            predB[idx, pos] = -1\n",
    "            continue\n",
    "\n",
    "        ion_type, ion_charge, cut_pos = frag_ion_names[pos]\n",
    "        if ion_type == 'a' or ion_type == 'b':\n",
    "            pre_suf_fix = seq[:cut_pos]\n",
    "        else:\n",
    "            pre_suf_fix = seq[-cut_pos:]\n",
    "        keyA = (ion_type, ion_charge)\n",
    "        keyB = (ion_type, ion_charge, pre_suf_fix)\n",
    "        if keyA in estimA0:\n",
    "            predA[idx, pos] = estimA0[keyA]\n",
    "        if keyB in estimB0:\n",
    "            predB[idx, pos] = estimB0[keyB]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d35dd6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator A predictions saved to:  ./predictions/3_estimator_A.npy\n",
      "Estimator B predictions saved to:  ./predictions/3_estimator_B.npy\n"
     ]
    }
   ],
   "source": [
    "# save the whole results for later evaluation\n",
    "\n",
    "np.save(predictions_folder+'estimator_A'+'.npy', predA) \n",
    "np.save(predictions_folder+'estimator_B'+'.npy', predB)\n",
    "# predictions_folder\n",
    "\n",
    "print(\"Estimator A predictions saved to: \", predictions_folder+'estimator_A'+'.npy')\n",
    "print(\"Estimator B predictions saved to: \", predictions_folder+'estimator_B'+'.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f989c8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean L1 loss of estimator A: 0.2400692086089979\n",
      "Mean L1 loss of estimator B: 0.17887376667234345\n",
      "MSE loss of estimator A: 0.10078319603828337\n",
      "MSE loss of estimator B: 0.11863857940326544\n"
     ]
    }
   ],
   "source": [
    "# mean l1 and MSE loss on test set\n",
    "tmp_mask = (Y_test >= -.1)\n",
    "\n",
    "lossA = np.abs(predA[test_mask][tmp_mask] - Y_test[tmp_mask]).mean()\n",
    "lossB = np.abs(predB[test_mask][tmp_mask] - Y_test[tmp_mask]).mean()\n",
    "\n",
    "mseA = ((predA[test_mask][tmp_mask] - Y_test[tmp_mask])**2).mean()\n",
    "mseB = ((predB[test_mask][tmp_mask] - Y_test[tmp_mask])**2).mean()\n",
    "\n",
    "\n",
    "print(f\"Mean L1 loss of estimator A: {lossA}\")\n",
    "print(f\"Mean L1 loss of estimator B: {lossB}\")\n",
    "\n",
    "print(f\"MSE loss of estimator A: {mseA}\")\n",
    "print(f\"MSE loss of estimator B: {mseB}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4e8bda",
   "metadata": {},
   "source": [
    "## Linear regression\n",
    "\n",
    "For each element in Y, set up an independent linear regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "44543402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regressor 0 trained.\n",
      "Regressor 1 trained.\n",
      "Regressor 2 trained.\n",
      "Regressor 3 trained.\n",
      "Regressor 4 trained.\n",
      "Regressor 5 trained.\n",
      "Regressor 6 trained.\n",
      "Regressor 7 trained.\n",
      "Regressor 8 trained.\n",
      "Regressor 9 trained.\n",
      "Regressor 10 trained.\n",
      "Regressor 11 trained.\n",
      "Regressor 12 trained.\n",
      "Regressor 13 trained.\n",
      "Regressor 14 trained.\n",
      "Regressor 15 trained.\n",
      "Regressor 16 trained.\n",
      "Regressor 17 trained.\n",
      "Regressor 18 trained.\n",
      "Regressor 19 trained.\n",
      "Regressor 20 trained.\n",
      "Regressor 21 trained.\n",
      "Regressor 22 trained.\n",
      "Regressor 23 trained.\n",
      "Regressor 24 trained.\n",
      "Regressor 25 trained.\n",
      "Regressor 26 trained.\n",
      "Regressor 27 trained.\n",
      "Regressor 28 trained.\n",
      "Regressor 29 trained.\n",
      "Regressor 30 trained.\n",
      "Regressor 31 trained.\n",
      "Regressor 32 trained.\n",
      "Regressor 33 trained.\n",
      "Regressor 34 trained.\n",
      "Regressor 35 trained.\n",
      "Regressor 36 trained.\n",
      "Regressor 37 trained.\n",
      "Regressor 38 trained.\n",
      "Regressor 39 trained.\n",
      "Regressor 40 trained.\n",
      "Regressor 41 trained.\n",
      "Regressor 42 trained.\n",
      "Regressor 43 trained.\n",
      "Regressor 44 trained.\n",
      "Regressor 45 trained.\n",
      "Regressor 46 trained.\n",
      "Regressor 47 trained.\n",
      "Regressor 48 trained.\n",
      "Regressor 49 trained.\n",
      "Regressor 50 trained.\n",
      "Regressor 51 trained.\n",
      "Regressor 52 trained.\n",
      "Regressor 53 trained.\n",
      "Regressor 54 trained.\n",
      "Regressor 55 trained.\n",
      "Regressor 56 trained.\n",
      "Regressor 57 trained.\n",
      "Regressor 58 trained.\n",
      "Regressor 59 trained.\n",
      "Regressor 60 trained.\n",
      "Regressor 61 trained.\n",
      "Regressor 62 trained.\n",
      "Regressor 63 trained.\n",
      "Regressor 64 trained.\n",
      "Regressor 65 trained.\n",
      "Regressor 66 trained.\n",
      "Regressor 67 trained.\n",
      "Regressor 68 trained.\n",
      "Regressor 69 trained.\n",
      "Regressor 70 trained.\n",
      "Regressor 71 trained.\n",
      "Regressor 72 trained.\n",
      "Regressor 73 trained.\n",
      "Regressor 74 trained.\n",
      "Regressor 75 trained.\n",
      "Regressor 76 trained.\n",
      "Regressor 77 trained.\n",
      "Regressor 78 trained.\n",
      "Regressor 79 trained.\n",
      "Regressor 80 trained.\n",
      "Regressor 81 trained.\n",
      "Regressor 82 trained.\n",
      "Regressor 83 trained.\n",
      "Regressor 84 trained.\n",
      "Regressor 85 trained.\n",
      "Regressor 86 trained.\n",
      "Regressor 87 trained.\n",
      "Regressor 88 trained.\n",
      "Regressor 89 trained.\n",
      "Regressor 90 trained.\n",
      "Regressor 91 trained.\n",
      "Regressor 92 trained.\n",
      "Regressor 93 trained.\n",
      "Regressor 94 trained.\n",
      "Regressor 95 trained.\n",
      "Regressor 96 trained.\n",
      "Regressor 97 trained.\n",
      "Regressor 98 trained.\n",
      "Regressor 99 trained.\n",
      "Regressor 100 trained.\n",
      "Regressor 101 trained.\n",
      "Regressor 102 trained.\n",
      "Regressor 103 trained.\n",
      "Regressor 104 trained.\n",
      "Regressor 105 trained.\n",
      "Regressor 106 trained.\n",
      "Regressor 107 trained.\n",
      "Regressor 108 trained.\n",
      "Regressor 109 trained.\n",
      "Regressor 110 trained.\n",
      "Regressor 111 trained.\n",
      "Regressor 112 trained.\n",
      "Regressor 113 trained.\n",
      "Regressor 114 trained.\n",
      "Regressor 115 trained.\n",
      "Regressor 116 trained.\n",
      "Regressor 117 trained.\n",
      "Regressor 118 trained.\n",
      "Regressor 119 trained.\n",
      "Regressor 120 trained.\n",
      "Regressor 121 trained.\n",
      "Regressor 122 trained.\n",
      "Regressor 123 trained.\n",
      "Regressor 124 trained.\n",
      "Regressor 125 trained.\n",
      "Regressor 126 trained.\n",
      "Regressor 127 trained.\n",
      "Regressor 128 trained.\n",
      "Regressor 129 trained.\n",
      "Regressor 130 trained.\n",
      "Regressor 131 trained.\n",
      "Regressor 132 trained.\n",
      "Regressor 133 trained.\n",
      "Regressor 134 trained.\n",
      "Regressor 135 trained.\n",
      "Regressor 136 trained.\n",
      "Regressor 137 trained.\n",
      "Regressor 138 trained.\n",
      "Regressor 139 trained.\n",
      "Regressor 140 trained.\n",
      "Regressor 141 trained.\n",
      "Regressor 142 trained.\n",
      "Regressor 143 trained.\n",
      "Regressor 144 trained.\n",
      "Regressor 145 trained.\n",
      "Regressor 146 trained.\n",
      "Regressor 147 trained.\n",
      "Regressor 148 trained.\n",
      "Regressor 149 trained.\n",
      "Regressor 150 trained.\n",
      "Regressor 151 trained.\n",
      "Regressor 152 trained.\n",
      "Regressor 153 trained.\n",
      "Regressor 154 trained.\n",
      "Regressor 155 trained.\n",
      "Regressor 156 trained.\n",
      "Regressor 157 trained.\n",
      "Regressor 158 trained.\n",
      "Regressor 159 trained.\n",
      "Regressor 160 trained.\n",
      "Regressor 161 trained.\n",
      "Regressor 162 trained.\n",
      "Regressor 163 trained.\n",
      "Regressor 164 trained.\n",
      "Regressor 165 trained.\n",
      "Regressor 166 trained.\n",
      "Regressor 167 trained.\n",
      "Regressor 168 trained.\n",
      "Regressor 169 trained.\n",
      "Regressor 170 trained.\n",
      "Regressor 171 trained.\n",
      "Regressor 172 trained.\n",
      "Regressor 173 trained.\n",
      "Regressor 174 trained.\n",
      "Regressor 175 trained.\n",
      "Regressor 176 trained.\n",
      "Regressor 177 trained.\n",
      "Regressor 178 trained.\n",
      "Regressor 179 trained.\n",
      "Regressor 180 trained.\n",
      "Regressor 181 trained.\n",
      "Regressor 182 trained.\n",
      "Regressor 183 trained.\n",
      "Regressor 184 trained.\n",
      "Regressor 185 trained.\n",
      "Regressor 186 trained.\n",
      "Regressor 187 trained.\n",
      "Regressor 188 trained.\n",
      "Regressor 189 trained.\n",
      "Regressor 190 trained.\n",
      "Regressor 191 trained.\n",
      "Regressor 192 trained.\n",
      "Regressor 193 trained.\n",
      "Regressor 194 trained.\n",
      "Regressor 195 trained.\n",
      "Regressor 196 trained.\n",
      "Regressor 197 trained.\n",
      "Regressor 198 trained.\n",
      "Regressor 199 trained.\n",
      "Regressor 200 trained.\n",
      "Regressor 201 trained.\n",
      "Regressor 202 trained.\n",
      "Regressor 203 trained.\n",
      "Regressor 204 trained.\n",
      "Regressor 205 trained.\n",
      "Regressor 206 trained.\n",
      "Regressor 207 trained.\n",
      "Regressor 208 trained.\n",
      "Regressor 209 trained.\n",
      "Regressor 210 trained.\n",
      "Regressor 211 trained.\n",
      "Regressor 212 trained.\n",
      "Regressor 213 trained.\n",
      "Regressor 214 trained.\n",
      "Regressor 215 trained.\n",
      "Regressor 216 trained.\n",
      "Regressor 217 trained.\n",
      "Regressor 218 trained.\n",
      "Regressor 219 trained.\n",
      "Regressor 220 trained.\n",
      "Regressor 221 trained.\n",
      "Regressor 222 trained.\n",
      "Regressor 223 trained.\n",
      "Regressor 224 trained.\n",
      "Regressor 225 trained.\n",
      "Regressor 226 trained.\n",
      "Regressor 227 trained.\n",
      "Regressor 228 trained.\n",
      "Regressor 229 trained.\n",
      "Regressor 230 trained.\n",
      "Regressor 231 trained.\n",
      "Regressor 232 trained.\n",
      "Regressor 233 trained.\n",
      "Regressor 234 trained.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "# Initialize a list to store the regressors\n",
    "regressors = []\n",
    "\n",
    "# Train a separate regressor for each column in Y\n",
    "for i in range(Y_train.shape[1]):\n",
    "    regressor = SGDRegressor(loss='epsilon_insensitive', epsilon=0, random_state=seed)\n",
    "    regressor.fit(X_train, Y_train[:, i])\n",
    "    regressors.append(regressor)\n",
    "    print(f\"Regressor {i} trained.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "10ffa7c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regression predictions saved to:  ./predictions/3_linear_regression.npy\n"
     ]
    }
   ],
   "source": [
    "# predict on the whole dataset\n",
    "linear_pred = np.zeros((Y.shape[0], Y.shape[1]))\n",
    "for i in range(Y.shape[1]):\n",
    "    linear_pred[:, i] = regressors[i].predict(X_1hot)\n",
    "\n",
    "# save the whole results for later evaluation\n",
    "np.save(predictions_folder+'linear_regression'+'.npy', linear_pred)\n",
    "print(\"Linear regression predictions saved to: \", predictions_folder+'linear_regression'+'.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1733be08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean L1 loss of linear regression: 0.1341388241523988\n",
      "MSE loss of linear regression: 0.05803715006853861\n"
     ]
    }
   ],
   "source": [
    "# compute l1 and MSE loss on test set\n",
    "tmp_mask = (Y_test >= -.1)\n",
    "l1_linear = np.abs(linear_pred[test_mask][tmp_mask] - Y_test[tmp_mask]).mean()\n",
    "mse_linear = ((linear_pred[test_mask][tmp_mask] - Y_test[tmp_mask])**2).mean()\n",
    "\n",
    "print(f\"Mean L1 loss of linear regression: {l1_linear}\")\n",
    "print(f\"MSE loss of linear regression: {mse_linear}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b081847f",
   "metadata": {},
   "source": [
    "## simple neural-network with skip connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7aa6047a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# fix random seed for torch\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 512\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "Y_train_tensor = torch.tensor(Y_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "Y_test_tensor = torch.tensor(Y_test, dtype=torch.float32)\n",
    "train_dataset = TensorDataset(X_train_tensor, Y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataset = TensorDataset(X_test_tensor, Y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4a14a9a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Step [100/554], Loss: 0.1615\n",
      "Epoch [1/200], Step [200/554], Loss: 0.1463\n",
      "Epoch [1/200], Step [300/554], Loss: 0.1446\n",
      "Epoch [1/200], Step [400/554], Loss: 0.1267\n",
      "Epoch [1/200], Step [500/554], Loss: 0.1213\n",
      "Epoch [1/200], Test Loss: 0.1148\n",
      "Epoch [2/200], Step [100/554], Loss: 0.1126\n",
      "Epoch [2/200], Step [200/554], Loss: 0.1089\n",
      "Epoch [2/200], Step [300/554], Loss: 0.1061\n",
      "Epoch [2/200], Step [400/554], Loss: 0.1061\n",
      "Epoch [2/200], Step [500/554], Loss: 0.1028\n",
      "Epoch [2/200], Test Loss: 0.1017\n",
      "Epoch [3/200], Step [100/554], Loss: 0.1008\n",
      "Epoch [3/200], Step [200/554], Loss: 0.0991\n",
      "Epoch [3/200], Step [300/554], Loss: 0.0990\n",
      "Epoch [3/200], Step [400/554], Loss: 0.0997\n",
      "Epoch [3/200], Step [500/554], Loss: 0.0977\n",
      "Epoch [3/200], Test Loss: 0.0967\n",
      "Epoch [4/200], Step [100/554], Loss: 0.0968\n",
      "Epoch [4/200], Step [200/554], Loss: 0.0945\n",
      "Epoch [4/200], Step [300/554], Loss: 0.0964\n",
      "Epoch [4/200], Step [400/554], Loss: 0.0968\n",
      "Epoch [4/200], Step [500/554], Loss: 0.0960\n",
      "Epoch [4/200], Test Loss: 0.0937\n",
      "Epoch [5/200], Step [100/554], Loss: 0.0947\n",
      "Epoch [5/200], Step [200/554], Loss: 0.0963\n",
      "Epoch [5/200], Step [300/554], Loss: 0.0918\n",
      "Epoch [5/200], Step [400/554], Loss: 0.0946\n",
      "Epoch [5/200], Step [500/554], Loss: 0.0914\n",
      "Epoch [5/200], Test Loss: 0.0918\n",
      "Epoch [6/200], Step [100/554], Loss: 0.0914\n",
      "Epoch [6/200], Step [200/554], Loss: 0.0907\n",
      "Epoch [6/200], Step [300/554], Loss: 0.0894\n",
      "Epoch [6/200], Step [400/554], Loss: 0.0929\n",
      "Epoch [6/200], Step [500/554], Loss: 0.0916\n",
      "Epoch [6/200], Test Loss: 0.0903\n",
      "Epoch [7/200], Step [100/554], Loss: 0.0910\n",
      "Epoch [7/200], Step [200/554], Loss: 0.0910\n",
      "Epoch [7/200], Step [300/554], Loss: 0.0890\n",
      "Epoch [7/200], Step [400/554], Loss: 0.0902\n",
      "Epoch [7/200], Step [500/554], Loss: 0.0936\n",
      "Epoch [7/200], Test Loss: 0.0893\n",
      "Epoch [8/200], Step [100/554], Loss: 0.0895\n",
      "Epoch [8/200], Step [200/554], Loss: 0.0892\n",
      "Epoch [8/200], Step [300/554], Loss: 0.0914\n",
      "Epoch [8/200], Step [400/554], Loss: 0.0904\n",
      "Epoch [8/200], Step [500/554], Loss: 0.0885\n",
      "Epoch [8/200], Test Loss: 0.0883\n",
      "Epoch [9/200], Step [100/554], Loss: 0.0884\n",
      "Epoch [9/200], Step [200/554], Loss: 0.0891\n",
      "Epoch [9/200], Step [300/554], Loss: 0.0900\n",
      "Epoch [9/200], Step [400/554], Loss: 0.0896\n",
      "Epoch [9/200], Step [500/554], Loss: 0.0886\n",
      "Epoch [9/200], Test Loss: 0.0875\n",
      "Epoch [10/200], Step [100/554], Loss: 0.0881\n",
      "Epoch [10/200], Step [200/554], Loss: 0.0852\n",
      "Epoch [10/200], Step [300/554], Loss: 0.0906\n",
      "Epoch [10/200], Step [400/554], Loss: 0.0874\n",
      "Epoch [10/200], Step [500/554], Loss: 0.0884\n",
      "Epoch [10/200], Test Loss: 0.0868\n",
      "Epoch [11/200], Step [100/554], Loss: 0.0877\n",
      "Epoch [11/200], Step [200/554], Loss: 0.0871\n",
      "Epoch [11/200], Step [300/554], Loss: 0.0896\n",
      "Epoch [11/200], Step [400/554], Loss: 0.0863\n",
      "Epoch [11/200], Step [500/554], Loss: 0.0857\n",
      "Epoch [11/200], Test Loss: 0.0863\n",
      "Epoch [12/200], Step [100/554], Loss: 0.0852\n",
      "Epoch [12/200], Step [200/554], Loss: 0.0825\n",
      "Epoch [12/200], Step [300/554], Loss: 0.0876\n",
      "Epoch [12/200], Step [400/554], Loss: 0.0869\n",
      "Epoch [12/200], Step [500/554], Loss: 0.0885\n",
      "Epoch [12/200], Test Loss: 0.0859\n",
      "Epoch [13/200], Step [100/554], Loss: 0.0871\n",
      "Epoch [13/200], Step [200/554], Loss: 0.0860\n",
      "Epoch [13/200], Step [300/554], Loss: 0.0877\n",
      "Epoch [13/200], Step [400/554], Loss: 0.0871\n",
      "Epoch [13/200], Step [500/554], Loss: 0.0856\n",
      "Epoch [13/200], Test Loss: 0.0854\n",
      "Epoch [14/200], Step [100/554], Loss: 0.0866\n",
      "Epoch [14/200], Step [200/554], Loss: 0.0865\n",
      "Epoch [14/200], Step [300/554], Loss: 0.0836\n",
      "Epoch [14/200], Step [400/554], Loss: 0.0847\n",
      "Epoch [14/200], Step [500/554], Loss: 0.0842\n",
      "Epoch [14/200], Test Loss: 0.0852\n",
      "Epoch [15/200], Step [100/554], Loss: 0.0839\n",
      "Epoch [15/200], Step [200/554], Loss: 0.0877\n",
      "Epoch [15/200], Step [300/554], Loss: 0.0857\n",
      "Epoch [15/200], Step [400/554], Loss: 0.0838\n",
      "Epoch [15/200], Step [500/554], Loss: 0.0833\n",
      "Epoch [15/200], Test Loss: 0.0847\n",
      "Epoch [16/200], Step [100/554], Loss: 0.0854\n",
      "Epoch [16/200], Step [200/554], Loss: 0.0862\n",
      "Epoch [16/200], Step [300/554], Loss: 0.0840\n",
      "Epoch [16/200], Step [400/554], Loss: 0.0844\n",
      "Epoch [16/200], Step [500/554], Loss: 0.0864\n",
      "Epoch [16/200], Test Loss: 0.0845\n",
      "Epoch [17/200], Step [100/554], Loss: 0.0831\n",
      "Epoch [17/200], Step [200/554], Loss: 0.0846\n",
      "Epoch [17/200], Step [300/554], Loss: 0.0824\n",
      "Epoch [17/200], Step [400/554], Loss: 0.0853\n",
      "Epoch [17/200], Step [500/554], Loss: 0.0836\n",
      "Epoch [17/200], Test Loss: 0.0841\n",
      "Epoch [18/200], Step [100/554], Loss: 0.0832\n",
      "Epoch [18/200], Step [200/554], Loss: 0.0822\n",
      "Epoch [18/200], Step [300/554], Loss: 0.0824\n",
      "Epoch [18/200], Step [400/554], Loss: 0.0848\n",
      "Epoch [18/200], Step [500/554], Loss: 0.0850\n",
      "Epoch [18/200], Test Loss: 0.0839\n",
      "Epoch [19/200], Step [100/554], Loss: 0.0811\n",
      "Epoch [19/200], Step [200/554], Loss: 0.0813\n",
      "Epoch [19/200], Step [300/554], Loss: 0.0830\n",
      "Epoch [19/200], Step [400/554], Loss: 0.0846\n",
      "Epoch [19/200], Step [500/554], Loss: 0.0848\n",
      "Epoch [19/200], Test Loss: 0.0836\n",
      "Epoch [20/200], Step [100/554], Loss: 0.0837\n",
      "Epoch [20/200], Step [200/554], Loss: 0.0827\n",
      "Epoch [20/200], Step [300/554], Loss: 0.0830\n",
      "Epoch [20/200], Step [400/554], Loss: 0.0826\n",
      "Epoch [20/200], Step [500/554], Loss: 0.0830\n",
      "Epoch [20/200], Test Loss: 0.0834\n",
      "Epoch [21/200], Step [100/554], Loss: 0.0841\n",
      "Epoch [21/200], Step [200/554], Loss: 0.0858\n",
      "Epoch [21/200], Step [300/554], Loss: 0.0830\n",
      "Epoch [21/200], Step [400/554], Loss: 0.0847\n",
      "Epoch [21/200], Step [500/554], Loss: 0.0826\n",
      "Epoch [21/200], Test Loss: 0.0831\n",
      "Epoch [22/200], Step [100/554], Loss: 0.0832\n",
      "Epoch [22/200], Step [200/554], Loss: 0.0833\n",
      "Epoch [22/200], Step [300/554], Loss: 0.0844\n",
      "Epoch [22/200], Step [400/554], Loss: 0.0830\n",
      "Epoch [22/200], Step [500/554], Loss: 0.0850\n",
      "Epoch [22/200], Test Loss: 0.0830\n",
      "Epoch [23/200], Step [100/554], Loss: 0.0819\n",
      "Epoch [23/200], Step [200/554], Loss: 0.0801\n",
      "Epoch [23/200], Step [300/554], Loss: 0.0814\n",
      "Epoch [23/200], Step [400/554], Loss: 0.0806\n",
      "Epoch [23/200], Step [500/554], Loss: 0.0827\n",
      "Epoch [23/200], Test Loss: 0.0827\n",
      "Epoch [24/200], Step [100/554], Loss: 0.0801\n",
      "Epoch [24/200], Step [200/554], Loss: 0.0823\n",
      "Epoch [24/200], Step [300/554], Loss: 0.0820\n",
      "Epoch [24/200], Step [400/554], Loss: 0.0808\n",
      "Epoch [24/200], Step [500/554], Loss: 0.0821\n",
      "Epoch [24/200], Test Loss: 0.0825\n",
      "Epoch [25/200], Step [100/554], Loss: 0.0832\n",
      "Epoch [25/200], Step [200/554], Loss: 0.0827\n",
      "Epoch [25/200], Step [300/554], Loss: 0.0804\n",
      "Epoch [25/200], Step [400/554], Loss: 0.0837\n",
      "Epoch [25/200], Step [500/554], Loss: 0.0830\n",
      "Epoch [25/200], Test Loss: 0.0822\n",
      "Epoch [26/200], Step [100/554], Loss: 0.0799\n",
      "Epoch [26/200], Step [200/554], Loss: 0.0829\n",
      "Epoch [26/200], Step [300/554], Loss: 0.0810\n",
      "Epoch [26/200], Step [400/554], Loss: 0.0820\n",
      "Epoch [26/200], Step [500/554], Loss: 0.0825\n",
      "Epoch [26/200], Test Loss: 0.0820\n",
      "Epoch [27/200], Step [100/554], Loss: 0.0820\n",
      "Epoch [27/200], Step [200/554], Loss: 0.0831\n",
      "Epoch [27/200], Step [300/554], Loss: 0.0823\n",
      "Epoch [27/200], Step [400/554], Loss: 0.0843\n",
      "Epoch [27/200], Step [500/554], Loss: 0.0818\n",
      "Epoch [27/200], Test Loss: 0.0820\n",
      "Epoch [28/200], Step [100/554], Loss: 0.0822\n",
      "Epoch [28/200], Step [200/554], Loss: 0.0776\n",
      "Epoch [28/200], Step [300/554], Loss: 0.0825\n",
      "Epoch [28/200], Step [400/554], Loss: 0.0811\n",
      "Epoch [28/200], Step [500/554], Loss: 0.0832\n",
      "Epoch [28/200], Test Loss: 0.0818\n",
      "Epoch [29/200], Step [100/554], Loss: 0.0800\n",
      "Epoch [29/200], Step [200/554], Loss: 0.0809\n",
      "Epoch [29/200], Step [300/554], Loss: 0.0816\n",
      "Epoch [29/200], Step [400/554], Loss: 0.0801\n",
      "Epoch [29/200], Step [500/554], Loss: 0.0805\n",
      "Epoch [29/200], Test Loss: 0.0816\n",
      "Epoch [30/200], Step [100/554], Loss: 0.0803\n",
      "Epoch [30/200], Step [200/554], Loss: 0.0794\n",
      "Epoch [30/200], Step [300/554], Loss: 0.0813\n",
      "Epoch [30/200], Step [400/554], Loss: 0.0814\n",
      "Epoch [30/200], Step [500/554], Loss: 0.0827\n",
      "Epoch [30/200], Test Loss: 0.0814\n",
      "Epoch [31/200], Step [100/554], Loss: 0.0814\n",
      "Epoch [31/200], Step [200/554], Loss: 0.0790\n",
      "Epoch [31/200], Step [300/554], Loss: 0.0804\n",
      "Epoch [31/200], Step [400/554], Loss: 0.0818\n",
      "Epoch [31/200], Step [500/554], Loss: 0.0802\n",
      "Epoch [31/200], Test Loss: 0.0812\n",
      "Epoch [32/200], Step [100/554], Loss: 0.0809\n",
      "Epoch [32/200], Step [200/554], Loss: 0.0808\n",
      "Epoch [32/200], Step [300/554], Loss: 0.0796\n",
      "Epoch [32/200], Step [400/554], Loss: 0.0809\n",
      "Epoch [32/200], Step [500/554], Loss: 0.0803\n",
      "Epoch [32/200], Test Loss: 0.0811\n",
      "Epoch [33/200], Step [100/554], Loss: 0.0803\n",
      "Epoch [33/200], Step [200/554], Loss: 0.0783\n",
      "Epoch [33/200], Step [300/554], Loss: 0.0791\n",
      "Epoch [33/200], Step [400/554], Loss: 0.0799\n",
      "Epoch [33/200], Step [500/554], Loss: 0.0811\n",
      "Epoch [33/200], Test Loss: 0.0809\n",
      "Epoch [34/200], Step [100/554], Loss: 0.0786\n",
      "Epoch [34/200], Step [200/554], Loss: 0.0804\n",
      "Epoch [34/200], Step [300/554], Loss: 0.0795\n",
      "Epoch [34/200], Step [400/554], Loss: 0.0799\n",
      "Epoch [34/200], Step [500/554], Loss: 0.0795\n",
      "Epoch [34/200], Test Loss: 0.0808\n",
      "Epoch [35/200], Step [100/554], Loss: 0.0788\n",
      "Epoch [35/200], Step [200/554], Loss: 0.0816\n",
      "Epoch [35/200], Step [300/554], Loss: 0.0814\n",
      "Epoch [35/200], Step [400/554], Loss: 0.0793\n",
      "Epoch [35/200], Step [500/554], Loss: 0.0808\n",
      "Epoch [35/200], Test Loss: 0.0807\n",
      "Epoch [36/200], Step [100/554], Loss: 0.0813\n",
      "Epoch [36/200], Step [200/554], Loss: 0.0801\n",
      "Epoch [36/200], Step [300/554], Loss: 0.0818\n",
      "Epoch [36/200], Step [400/554], Loss: 0.0795\n",
      "Epoch [36/200], Step [500/554], Loss: 0.0806\n",
      "Epoch [36/200], Test Loss: 0.0806\n",
      "Epoch [37/200], Step [100/554], Loss: 0.0787\n",
      "Epoch [37/200], Step [200/554], Loss: 0.0801\n",
      "Epoch [37/200], Step [300/554], Loss: 0.0797\n",
      "Epoch [37/200], Step [400/554], Loss: 0.0808\n",
      "Epoch [37/200], Step [500/554], Loss: 0.0809\n",
      "Epoch [37/200], Test Loss: 0.0803\n",
      "Epoch [38/200], Step [100/554], Loss: 0.0799\n",
      "Epoch [38/200], Step [200/554], Loss: 0.0807\n",
      "Epoch [38/200], Step [300/554], Loss: 0.0798\n",
      "Epoch [38/200], Step [400/554], Loss: 0.0811\n",
      "Epoch [38/200], Step [500/554], Loss: 0.0824\n",
      "Epoch [38/200], Test Loss: 0.0802\n",
      "Epoch [39/200], Step [100/554], Loss: 0.0788\n",
      "Epoch [39/200], Step [200/554], Loss: 0.0797\n",
      "Epoch [39/200], Step [300/554], Loss: 0.0810\n",
      "Epoch [39/200], Step [400/554], Loss: 0.0805\n",
      "Epoch [39/200], Step [500/554], Loss: 0.0805\n",
      "Epoch [39/200], Test Loss: 0.0801\n",
      "Epoch [40/200], Step [100/554], Loss: 0.0806\n",
      "Epoch [40/200], Step [200/554], Loss: 0.0800\n",
      "Epoch [40/200], Step [300/554], Loss: 0.0774\n",
      "Epoch [40/200], Step [400/554], Loss: 0.0789\n",
      "Epoch [40/200], Step [500/554], Loss: 0.0767\n",
      "Epoch [40/200], Test Loss: 0.0800\n",
      "Epoch [41/200], Step [100/554], Loss: 0.0798\n",
      "Epoch [41/200], Step [200/554], Loss: 0.0802\n",
      "Epoch [41/200], Step [300/554], Loss: 0.0788\n",
      "Epoch [41/200], Step [400/554], Loss: 0.0812\n",
      "Epoch [41/200], Step [500/554], Loss: 0.0771\n",
      "Epoch [41/200], Test Loss: 0.0799\n",
      "Epoch [42/200], Step [100/554], Loss: 0.0767\n",
      "Epoch [42/200], Step [200/554], Loss: 0.0804\n",
      "Epoch [42/200], Step [300/554], Loss: 0.0778\n",
      "Epoch [42/200], Step [400/554], Loss: 0.0788\n",
      "Epoch [42/200], Step [500/554], Loss: 0.0786\n",
      "Epoch [42/200], Test Loss: 0.0798\n",
      "Epoch [43/200], Step [100/554], Loss: 0.0786\n",
      "Epoch [43/200], Step [200/554], Loss: 0.0795\n",
      "Epoch [43/200], Step [300/554], Loss: 0.0798\n",
      "Epoch [43/200], Step [400/554], Loss: 0.0804\n",
      "Epoch [43/200], Step [500/554], Loss: 0.0776\n",
      "Epoch [43/200], Test Loss: 0.0797\n",
      "Epoch [44/200], Step [100/554], Loss: 0.0766\n",
      "Epoch [44/200], Step [200/554], Loss: 0.0788\n",
      "Epoch [44/200], Step [300/554], Loss: 0.0784\n",
      "Epoch [44/200], Step [400/554], Loss: 0.0773\n",
      "Epoch [44/200], Step [500/554], Loss: 0.0765\n",
      "Epoch [44/200], Test Loss: 0.0796\n",
      "Epoch [45/200], Step [100/554], Loss: 0.0781\n",
      "Epoch [45/200], Step [200/554], Loss: 0.0784\n",
      "Epoch [45/200], Step [300/554], Loss: 0.0781\n",
      "Epoch [45/200], Step [400/554], Loss: 0.0781\n",
      "Epoch [45/200], Step [500/554], Loss: 0.0788\n",
      "Epoch [45/200], Test Loss: 0.0795\n",
      "Epoch [46/200], Step [100/554], Loss: 0.0782\n",
      "Epoch [46/200], Step [200/554], Loss: 0.0784\n",
      "Epoch [46/200], Step [300/554], Loss: 0.0790\n",
      "Epoch [46/200], Step [400/554], Loss: 0.0795\n",
      "Epoch [46/200], Step [500/554], Loss: 0.0785\n",
      "Epoch [46/200], Test Loss: 0.0793\n",
      "Epoch [47/200], Step [100/554], Loss: 0.0787\n",
      "Epoch [47/200], Step [200/554], Loss: 0.0790\n",
      "Epoch [47/200], Step [300/554], Loss: 0.0785\n",
      "Epoch [47/200], Step [400/554], Loss: 0.0765\n",
      "Epoch [47/200], Step [500/554], Loss: 0.0782\n",
      "Epoch [47/200], Test Loss: 0.0793\n",
      "Epoch [48/200], Step [100/554], Loss: 0.0766\n",
      "Epoch [48/200], Step [200/554], Loss: 0.0781\n",
      "Epoch [48/200], Step [300/554], Loss: 0.0792\n",
      "Epoch [48/200], Step [400/554], Loss: 0.0803\n",
      "Epoch [48/200], Step [500/554], Loss: 0.0777\n",
      "Epoch [48/200], Test Loss: 0.0792\n",
      "Epoch [49/200], Step [100/554], Loss: 0.0776\n",
      "Epoch [49/200], Step [200/554], Loss: 0.0774\n",
      "Epoch [49/200], Step [300/554], Loss: 0.0791\n",
      "Epoch [49/200], Step [400/554], Loss: 0.0807\n",
      "Epoch [49/200], Step [500/554], Loss: 0.0789\n",
      "Epoch [49/200], Test Loss: 0.0791\n",
      "Epoch [50/200], Step [100/554], Loss: 0.0770\n",
      "Epoch [50/200], Step [200/554], Loss: 0.0790\n",
      "Epoch [50/200], Step [300/554], Loss: 0.0770\n",
      "Epoch [50/200], Step [400/554], Loss: 0.0766\n",
      "Epoch [50/200], Step [500/554], Loss: 0.0795\n",
      "Epoch [50/200], Test Loss: 0.0791\n",
      "Epoch [51/200], Step [100/554], Loss: 0.0792\n",
      "Epoch [51/200], Step [200/554], Loss: 0.0772\n",
      "Epoch [51/200], Step [300/554], Loss: 0.0777\n",
      "Epoch [51/200], Step [400/554], Loss: 0.0785\n",
      "Epoch [51/200], Step [500/554], Loss: 0.0776\n",
      "Epoch [51/200], Test Loss: 0.0790\n",
      "Epoch [52/200], Step [100/554], Loss: 0.0777\n",
      "Epoch [52/200], Step [200/554], Loss: 0.0774\n",
      "Epoch [52/200], Step [300/554], Loss: 0.0790\n",
      "Epoch [52/200], Step [400/554], Loss: 0.0790\n",
      "Epoch [52/200], Step [500/554], Loss: 0.0792\n",
      "Epoch [52/200], Test Loss: 0.0788\n",
      "Epoch [53/200], Step [100/554], Loss: 0.0765\n",
      "Epoch [53/200], Step [200/554], Loss: 0.0779\n",
      "Epoch [53/200], Step [300/554], Loss: 0.0773\n",
      "Epoch [53/200], Step [400/554], Loss: 0.0781\n",
      "Epoch [53/200], Step [500/554], Loss: 0.0787\n",
      "Epoch [53/200], Test Loss: 0.0787\n",
      "Epoch [54/200], Step [100/554], Loss: 0.0764\n",
      "Epoch [54/200], Step [200/554], Loss: 0.0763\n",
      "Epoch [54/200], Step [300/554], Loss: 0.0777\n",
      "Epoch [54/200], Step [400/554], Loss: 0.0787\n",
      "Epoch [54/200], Step [500/554], Loss: 0.0796\n",
      "Epoch [54/200], Test Loss: 0.0787\n",
      "Epoch [55/200], Step [100/554], Loss: 0.0773\n",
      "Epoch [55/200], Step [200/554], Loss: 0.0791\n",
      "Epoch [55/200], Step [300/554], Loss: 0.0762\n",
      "Epoch [55/200], Step [400/554], Loss: 0.0799\n",
      "Epoch [55/200], Step [500/554], Loss: 0.0785\n",
      "Epoch [55/200], Test Loss: 0.0785\n",
      "Epoch [56/200], Step [100/554], Loss: 0.0790\n",
      "Epoch [56/200], Step [200/554], Loss: 0.0784\n",
      "Epoch [56/200], Step [300/554], Loss: 0.0764\n",
      "Epoch [56/200], Step [400/554], Loss: 0.0791\n",
      "Epoch [56/200], Step [500/554], Loss: 0.0765\n",
      "Epoch [56/200], Test Loss: 0.0785\n",
      "Epoch [57/200], Step [100/554], Loss: 0.0776\n",
      "Epoch [57/200], Step [200/554], Loss: 0.0760\n",
      "Epoch [57/200], Step [300/554], Loss: 0.0766\n",
      "Epoch [57/200], Step [400/554], Loss: 0.0770\n",
      "Epoch [57/200], Step [500/554], Loss: 0.0780\n",
      "Epoch [57/200], Test Loss: 0.0785\n",
      "Epoch [58/200], Step [100/554], Loss: 0.0764\n",
      "Epoch [58/200], Step [200/554], Loss: 0.0785\n",
      "Epoch [58/200], Step [300/554], Loss: 0.0790\n",
      "Epoch [58/200], Step [400/554], Loss: 0.0772\n",
      "Epoch [58/200], Step [500/554], Loss: 0.0794\n",
      "Epoch [58/200], Test Loss: 0.0783\n",
      "Epoch [59/200], Step [100/554], Loss: 0.0754\n",
      "Epoch [59/200], Step [200/554], Loss: 0.0777\n",
      "Epoch [59/200], Step [300/554], Loss: 0.0791\n",
      "Epoch [59/200], Step [400/554], Loss: 0.0777\n",
      "Epoch [59/200], Step [500/554], Loss: 0.0768\n",
      "Epoch [59/200], Test Loss: 0.0783\n",
      "Epoch [60/200], Step [100/554], Loss: 0.0782\n",
      "Epoch [60/200], Step [200/554], Loss: 0.0773\n",
      "Epoch [60/200], Step [300/554], Loss: 0.0766\n",
      "Epoch [60/200], Step [400/554], Loss: 0.0767\n",
      "Epoch [60/200], Step [500/554], Loss: 0.0772\n",
      "Epoch [60/200], Test Loss: 0.0781\n",
      "Epoch [61/200], Step [100/554], Loss: 0.0778\n",
      "Epoch [61/200], Step [200/554], Loss: 0.0758\n",
      "Epoch [61/200], Step [300/554], Loss: 0.0764\n",
      "Epoch [61/200], Step [400/554], Loss: 0.0755\n",
      "Epoch [61/200], Step [500/554], Loss: 0.0768\n",
      "Epoch [61/200], Test Loss: 0.0781\n",
      "Epoch [62/200], Step [100/554], Loss: 0.0763\n",
      "Epoch [62/200], Step [200/554], Loss: 0.0760\n",
      "Epoch [62/200], Step [300/554], Loss: 0.0781\n",
      "Epoch [62/200], Step [400/554], Loss: 0.0752\n",
      "Epoch [62/200], Step [500/554], Loss: 0.0765\n",
      "Epoch [62/200], Test Loss: 0.0781\n",
      "Epoch [63/200], Step [100/554], Loss: 0.0764\n",
      "Epoch [63/200], Step [200/554], Loss: 0.0791\n",
      "Epoch [63/200], Step [300/554], Loss: 0.0768\n",
      "Epoch [63/200], Step [400/554], Loss: 0.0771\n",
      "Epoch [63/200], Step [500/554], Loss: 0.0765\n",
      "Epoch [63/200], Test Loss: 0.0780\n",
      "Epoch [64/200], Step [100/554], Loss: 0.0757\n",
      "Epoch [64/200], Step [200/554], Loss: 0.0766\n",
      "Epoch [64/200], Step [300/554], Loss: 0.0761\n",
      "Epoch [64/200], Step [400/554], Loss: 0.0766\n",
      "Epoch [64/200], Step [500/554], Loss: 0.0790\n",
      "Epoch [64/200], Test Loss: 0.0779\n",
      "Epoch [65/200], Step [100/554], Loss: 0.0765\n",
      "Epoch [65/200], Step [200/554], Loss: 0.0770\n",
      "Epoch [65/200], Step [300/554], Loss: 0.0779\n",
      "Epoch [65/200], Step [400/554], Loss: 0.0778\n",
      "Epoch [65/200], Step [500/554], Loss: 0.0767\n",
      "Epoch [65/200], Test Loss: 0.0779\n",
      "Epoch [66/200], Step [100/554], Loss: 0.0751\n",
      "Epoch [66/200], Step [200/554], Loss: 0.0759\n",
      "Epoch [66/200], Step [300/554], Loss: 0.0772\n",
      "Epoch [66/200], Step [400/554], Loss: 0.0771\n",
      "Epoch [66/200], Step [500/554], Loss: 0.0758\n",
      "Epoch [66/200], Test Loss: 0.0778\n",
      "Epoch [67/200], Step [100/554], Loss: 0.0796\n",
      "Epoch [67/200], Step [200/554], Loss: 0.0780\n",
      "Epoch [67/200], Step [300/554], Loss: 0.0749\n",
      "Epoch [67/200], Step [400/554], Loss: 0.0746\n",
      "Epoch [67/200], Step [500/554], Loss: 0.0765\n",
      "Epoch [67/200], Test Loss: 0.0778\n",
      "Epoch [68/200], Step [100/554], Loss: 0.0767\n",
      "Epoch [68/200], Step [200/554], Loss: 0.0776\n",
      "Epoch [68/200], Step [300/554], Loss: 0.0753\n",
      "Epoch [68/200], Step [400/554], Loss: 0.0778\n",
      "Epoch [68/200], Step [500/554], Loss: 0.0789\n",
      "Epoch [68/200], Test Loss: 0.0777\n",
      "Epoch [69/200], Step [100/554], Loss: 0.0767\n",
      "Epoch [69/200], Step [200/554], Loss: 0.0775\n",
      "Epoch [69/200], Step [300/554], Loss: 0.0756\n",
      "Epoch [69/200], Step [400/554], Loss: 0.0780\n",
      "Epoch [69/200], Step [500/554], Loss: 0.0770\n",
      "Epoch [69/200], Test Loss: 0.0776\n",
      "Epoch [70/200], Step [100/554], Loss: 0.0765\n",
      "Epoch [70/200], Step [200/554], Loss: 0.0780\n",
      "Epoch [70/200], Step [300/554], Loss: 0.0772\n",
      "Epoch [70/200], Step [400/554], Loss: 0.0769\n",
      "Epoch [70/200], Step [500/554], Loss: 0.0763\n",
      "Epoch [70/200], Test Loss: 0.0776\n",
      "Epoch [71/200], Step [100/554], Loss: 0.0765\n",
      "Epoch [71/200], Step [200/554], Loss: 0.0757\n",
      "Epoch [71/200], Step [300/554], Loss: 0.0771\n",
      "Epoch [71/200], Step [400/554], Loss: 0.0758\n",
      "Epoch [71/200], Step [500/554], Loss: 0.0774\n",
      "Epoch [71/200], Test Loss: 0.0777\n",
      "Epoch [72/200], Step [100/554], Loss: 0.0745\n",
      "Epoch [72/200], Step [200/554], Loss: 0.0758\n",
      "Epoch [72/200], Step [300/554], Loss: 0.0740\n",
      "Epoch [72/200], Step [400/554], Loss: 0.0756\n",
      "Epoch [72/200], Step [500/554], Loss: 0.0775\n",
      "Epoch [72/200], Test Loss: 0.0776\n",
      "Epoch [73/200], Step [100/554], Loss: 0.0753\n",
      "Epoch [73/200], Step [200/554], Loss: 0.0766\n",
      "Epoch [73/200], Step [300/554], Loss: 0.0762\n",
      "Epoch [73/200], Step [400/554], Loss: 0.0778\n",
      "Epoch [73/200], Step [500/554], Loss: 0.0783\n",
      "Epoch [73/200], Test Loss: 0.0774\n",
      "Epoch [74/200], Step [100/554], Loss: 0.0775\n",
      "Epoch [74/200], Step [200/554], Loss: 0.0740\n",
      "Epoch [74/200], Step [300/554], Loss: 0.0771\n",
      "Epoch [74/200], Step [400/554], Loss: 0.0789\n",
      "Epoch [74/200], Step [500/554], Loss: 0.0779\n",
      "Epoch [74/200], Test Loss: 0.0773\n",
      "Epoch [75/200], Step [100/554], Loss: 0.0788\n",
      "Epoch [75/200], Step [200/554], Loss: 0.0771\n",
      "Epoch [75/200], Step [300/554], Loss: 0.0751\n",
      "Epoch [75/200], Step [400/554], Loss: 0.0766\n",
      "Epoch [75/200], Step [500/554], Loss: 0.0769\n",
      "Epoch [75/200], Test Loss: 0.0774\n",
      "Epoch [76/200], Step [100/554], Loss: 0.0760\n",
      "Epoch [76/200], Step [200/554], Loss: 0.0769\n",
      "Epoch [76/200], Step [300/554], Loss: 0.0782\n",
      "Epoch [76/200], Step [400/554], Loss: 0.0759\n",
      "Epoch [76/200], Step [500/554], Loss: 0.0762\n",
      "Epoch [76/200], Test Loss: 0.0773\n",
      "Epoch [77/200], Step [100/554], Loss: 0.0753\n",
      "Epoch [77/200], Step [200/554], Loss: 0.0755\n",
      "Epoch [77/200], Step [300/554], Loss: 0.0779\n",
      "Epoch [77/200], Step [400/554], Loss: 0.0779\n",
      "Epoch [77/200], Step [500/554], Loss: 0.0768\n",
      "Epoch [77/200], Test Loss: 0.0773\n",
      "Epoch [78/200], Step [100/554], Loss: 0.0759\n",
      "Epoch [78/200], Step [200/554], Loss: 0.0764\n",
      "Epoch [78/200], Step [300/554], Loss: 0.0749\n",
      "Epoch [78/200], Step [400/554], Loss: 0.0761\n",
      "Epoch [78/200], Step [500/554], Loss: 0.0786\n",
      "Epoch [78/200], Test Loss: 0.0771\n",
      "Epoch [79/200], Step [100/554], Loss: 0.0759\n",
      "Epoch [79/200], Step [200/554], Loss: 0.0767\n",
      "Epoch [79/200], Step [300/554], Loss: 0.0758\n",
      "Epoch [79/200], Step [400/554], Loss: 0.0759\n",
      "Epoch [79/200], Step [500/554], Loss: 0.0756\n",
      "Epoch [79/200], Test Loss: 0.0773\n",
      "Epoch [80/200], Step [100/554], Loss: 0.0745\n",
      "Epoch [80/200], Step [200/554], Loss: 0.0764\n",
      "Epoch [80/200], Step [300/554], Loss: 0.0764\n",
      "Epoch [80/200], Step [400/554], Loss: 0.0756\n",
      "Epoch [80/200], Step [500/554], Loss: 0.0763\n",
      "Epoch [80/200], Test Loss: 0.0771\n",
      "Epoch [81/200], Step [100/554], Loss: 0.0763\n",
      "Epoch [81/200], Step [200/554], Loss: 0.0760\n",
      "Epoch [81/200], Step [300/554], Loss: 0.0773\n",
      "Epoch [81/200], Step [400/554], Loss: 0.0755\n",
      "Epoch [81/200], Step [500/554], Loss: 0.0750\n",
      "Epoch [81/200], Test Loss: 0.0772\n",
      "Epoch [82/200], Step [100/554], Loss: 0.0756\n",
      "Epoch [82/200], Step [200/554], Loss: 0.0749\n",
      "Epoch [82/200], Step [300/554], Loss: 0.0778\n",
      "Epoch [82/200], Step [400/554], Loss: 0.0778\n",
      "Epoch [82/200], Step [500/554], Loss: 0.0750\n",
      "Epoch [82/200], Test Loss: 0.0769\n",
      "Epoch [83/200], Step [100/554], Loss: 0.0785\n",
      "Epoch [83/200], Step [200/554], Loss: 0.0758\n",
      "Epoch [83/200], Step [300/554], Loss: 0.0767\n",
      "Epoch [83/200], Step [400/554], Loss: 0.0759\n",
      "Epoch [83/200], Step [500/554], Loss: 0.0764\n",
      "Epoch [83/200], Test Loss: 0.0770\n",
      "Epoch [84/200], Step [100/554], Loss: 0.0737\n",
      "Epoch [84/200], Step [200/554], Loss: 0.0757\n",
      "Epoch [84/200], Step [300/554], Loss: 0.0750\n",
      "Epoch [84/200], Step [400/554], Loss: 0.0760\n",
      "Epoch [84/200], Step [500/554], Loss: 0.0753\n",
      "Epoch [84/200], Test Loss: 0.0770\n",
      "Epoch [85/200], Step [100/554], Loss: 0.0741\n",
      "Epoch [85/200], Step [200/554], Loss: 0.0749\n",
      "Epoch [85/200], Step [300/554], Loss: 0.0758\n",
      "Epoch [85/200], Step [400/554], Loss: 0.0742\n",
      "Epoch [85/200], Step [500/554], Loss: 0.0758\n",
      "Epoch [85/200], Test Loss: 0.0770\n",
      "Epoch [86/200], Step [100/554], Loss: 0.0762\n",
      "Epoch [86/200], Step [200/554], Loss: 0.0757\n",
      "Epoch [86/200], Step [300/554], Loss: 0.0761\n",
      "Epoch [86/200], Step [400/554], Loss: 0.0768\n",
      "Epoch [86/200], Step [500/554], Loss: 0.0767\n",
      "Epoch [86/200], Test Loss: 0.0768\n",
      "Epoch [87/200], Step [100/554], Loss: 0.0751\n",
      "Epoch [87/200], Step [200/554], Loss: 0.0761\n",
      "Epoch [87/200], Step [300/554], Loss: 0.0752\n",
      "Epoch [87/200], Step [400/554], Loss: 0.0751\n",
      "Epoch [87/200], Step [500/554], Loss: 0.0745\n",
      "Epoch [87/200], Test Loss: 0.0769\n",
      "Epoch [88/200], Step [100/554], Loss: 0.0741\n",
      "Epoch [88/200], Step [200/554], Loss: 0.0756\n",
      "Epoch [88/200], Step [300/554], Loss: 0.0726\n",
      "Epoch [88/200], Step [400/554], Loss: 0.0752\n",
      "Epoch [88/200], Step [500/554], Loss: 0.0768\n",
      "Epoch [88/200], Test Loss: 0.0767\n",
      "Epoch [89/200], Step [100/554], Loss: 0.0728\n",
      "Epoch [89/200], Step [200/554], Loss: 0.0759\n",
      "Epoch [89/200], Step [300/554], Loss: 0.0775\n",
      "Epoch [89/200], Step [400/554], Loss: 0.0762\n",
      "Epoch [89/200], Step [500/554], Loss: 0.0760\n",
      "Epoch [89/200], Test Loss: 0.0768\n",
      "Epoch [90/200], Step [100/554], Loss: 0.0753\n",
      "Epoch [90/200], Step [200/554], Loss: 0.0748\n",
      "Epoch [90/200], Step [300/554], Loss: 0.0763\n",
      "Epoch [90/200], Step [400/554], Loss: 0.0774\n",
      "Epoch [90/200], Step [500/554], Loss: 0.0746\n",
      "Epoch [90/200], Test Loss: 0.0768\n",
      "Epoch [91/200], Step [100/554], Loss: 0.0765\n",
      "Epoch [91/200], Step [200/554], Loss: 0.0765\n",
      "Epoch [91/200], Step [300/554], Loss: 0.0764\n",
      "Epoch [91/200], Step [400/554], Loss: 0.0766\n",
      "Epoch [91/200], Step [500/554], Loss: 0.0737\n",
      "Epoch [91/200], Test Loss: 0.0767\n",
      "Epoch [92/200], Step [100/554], Loss: 0.0759\n",
      "Epoch [92/200], Step [200/554], Loss: 0.0758\n",
      "Epoch [92/200], Step [300/554], Loss: 0.0766\n",
      "Epoch [92/200], Step [400/554], Loss: 0.0739\n",
      "Epoch [92/200], Step [500/554], Loss: 0.0761\n",
      "Epoch [92/200], Test Loss: 0.0766\n",
      "Epoch [93/200], Step [100/554], Loss: 0.0744\n",
      "Epoch [93/200], Step [200/554], Loss: 0.0734\n",
      "Epoch [93/200], Step [300/554], Loss: 0.0746\n",
      "Epoch [93/200], Step [400/554], Loss: 0.0754\n",
      "Epoch [93/200], Step [500/554], Loss: 0.0764\n",
      "Epoch [93/200], Test Loss: 0.0766\n",
      "Epoch [94/200], Step [100/554], Loss: 0.0779\n",
      "Epoch [94/200], Step [200/554], Loss: 0.0740\n",
      "Epoch [94/200], Step [300/554], Loss: 0.0746\n",
      "Epoch [94/200], Step [400/554], Loss: 0.0762\n",
      "Epoch [94/200], Step [500/554], Loss: 0.0752\n",
      "Epoch [94/200], Test Loss: 0.0766\n",
      "Epoch [95/200], Step [100/554], Loss: 0.0763\n",
      "Epoch [95/200], Step [200/554], Loss: 0.0754\n",
      "Epoch [95/200], Step [300/554], Loss: 0.0751\n",
      "Epoch [95/200], Step [400/554], Loss: 0.0747\n",
      "Epoch [95/200], Step [500/554], Loss: 0.0748\n",
      "Epoch [95/200], Test Loss: 0.0765\n",
      "Epoch [96/200], Step [100/554], Loss: 0.0756\n",
      "Epoch [96/200], Step [200/554], Loss: 0.0750\n",
      "Epoch [96/200], Step [300/554], Loss: 0.0754\n",
      "Epoch [96/200], Step [400/554], Loss: 0.0751\n",
      "Epoch [96/200], Step [500/554], Loss: 0.0763\n",
      "Epoch [96/200], Test Loss: 0.0766\n",
      "Epoch [97/200], Step [100/554], Loss: 0.0745\n",
      "Epoch [97/200], Step [200/554], Loss: 0.0740\n",
      "Epoch [97/200], Step [300/554], Loss: 0.0757\n",
      "Epoch [97/200], Step [400/554], Loss: 0.0740\n",
      "Epoch [97/200], Step [500/554], Loss: 0.0749\n",
      "Epoch [97/200], Test Loss: 0.0765\n",
      "Epoch [98/200], Step [100/554], Loss: 0.0742\n",
      "Epoch [98/200], Step [200/554], Loss: 0.0761\n",
      "Epoch [98/200], Step [300/554], Loss: 0.0742\n",
      "Epoch [98/200], Step [400/554], Loss: 0.0749\n",
      "Epoch [98/200], Step [500/554], Loss: 0.0756\n",
      "Epoch [98/200], Test Loss: 0.0765\n",
      "Epoch [99/200], Step [100/554], Loss: 0.0737\n",
      "Epoch [99/200], Step [200/554], Loss: 0.0758\n",
      "Epoch [99/200], Step [300/554], Loss: 0.0736\n",
      "Epoch [99/200], Step [400/554], Loss: 0.0738\n",
      "Epoch [99/200], Step [500/554], Loss: 0.0757\n",
      "Epoch [99/200], Test Loss: 0.0764\n",
      "Epoch [100/200], Step [100/554], Loss: 0.0739\n",
      "Epoch [100/200], Step [200/554], Loss: 0.0737\n",
      "Epoch [100/200], Step [300/554], Loss: 0.0744\n",
      "Epoch [100/200], Step [400/554], Loss: 0.0764\n",
      "Epoch [100/200], Step [500/554], Loss: 0.0750\n",
      "Epoch [100/200], Test Loss: 0.0763\n",
      "Epoch [101/200], Step [100/554], Loss: 0.0741\n",
      "Epoch [101/200], Step [200/554], Loss: 0.0763\n",
      "Epoch [101/200], Step [300/554], Loss: 0.0768\n",
      "Epoch [101/200], Step [400/554], Loss: 0.0766\n",
      "Epoch [101/200], Step [500/554], Loss: 0.0740\n",
      "Epoch [101/200], Test Loss: 0.0764\n",
      "Epoch [102/200], Step [100/554], Loss: 0.0735\n",
      "Epoch [102/200], Step [200/554], Loss: 0.0779\n",
      "Epoch [102/200], Step [300/554], Loss: 0.0752\n",
      "Epoch [102/200], Step [400/554], Loss: 0.0735\n",
      "Epoch [102/200], Step [500/554], Loss: 0.0769\n",
      "Epoch [102/200], Test Loss: 0.0764\n",
      "Epoch [103/200], Step [100/554], Loss: 0.0749\n",
      "Epoch [103/200], Step [200/554], Loss: 0.0759\n",
      "Epoch [103/200], Step [300/554], Loss: 0.0758\n",
      "Epoch [103/200], Step [400/554], Loss: 0.0754\n",
      "Epoch [103/200], Step [500/554], Loss: 0.0749\n",
      "Epoch [103/200], Test Loss: 0.0763\n",
      "Epoch [104/200], Step [100/554], Loss: 0.0742\n",
      "Epoch [104/200], Step [200/554], Loss: 0.0735\n",
      "Epoch [104/200], Step [300/554], Loss: 0.0747\n",
      "Epoch [104/200], Step [400/554], Loss: 0.0738\n",
      "Epoch [104/200], Step [500/554], Loss: 0.0750\n",
      "Epoch [104/200], Test Loss: 0.0763\n",
      "Epoch [105/200], Step [100/554], Loss: 0.0752\n",
      "Epoch [105/200], Step [200/554], Loss: 0.0752\n",
      "Epoch [105/200], Step [300/554], Loss: 0.0754\n",
      "Epoch [105/200], Step [400/554], Loss: 0.0754\n",
      "Epoch [105/200], Step [500/554], Loss: 0.0769\n",
      "Epoch [105/200], Test Loss: 0.0762\n",
      "Epoch [106/200], Step [100/554], Loss: 0.0749\n",
      "Epoch [106/200], Step [200/554], Loss: 0.0737\n",
      "Epoch [106/200], Step [300/554], Loss: 0.0773\n",
      "Epoch [106/200], Step [400/554], Loss: 0.0751\n",
      "Epoch [106/200], Step [500/554], Loss: 0.0752\n",
      "Epoch [106/200], Test Loss: 0.0761\n",
      "Epoch [107/200], Step [100/554], Loss: 0.0753\n",
      "Epoch [107/200], Step [200/554], Loss: 0.0753\n",
      "Epoch [107/200], Step [300/554], Loss: 0.0743\n",
      "Epoch [107/200], Step [400/554], Loss: 0.0733\n",
      "Epoch [107/200], Step [500/554], Loss: 0.0746\n",
      "Epoch [107/200], Test Loss: 0.0762\n",
      "Epoch [108/200], Step [100/554], Loss: 0.0731\n",
      "Epoch [108/200], Step [200/554], Loss: 0.0748\n",
      "Epoch [108/200], Step [300/554], Loss: 0.0750\n",
      "Epoch [108/200], Step [400/554], Loss: 0.0751\n",
      "Epoch [108/200], Step [500/554], Loss: 0.0763\n",
      "Epoch [108/200], Test Loss: 0.0761\n",
      "Epoch [109/200], Step [100/554], Loss: 0.0736\n",
      "Epoch [109/200], Step [200/554], Loss: 0.0743\n",
      "Epoch [109/200], Step [300/554], Loss: 0.0753\n",
      "Epoch [109/200], Step [400/554], Loss: 0.0736\n",
      "Epoch [109/200], Step [500/554], Loss: 0.0754\n",
      "Epoch [109/200], Test Loss: 0.0760\n",
      "Epoch [110/200], Step [100/554], Loss: 0.0737\n",
      "Epoch [110/200], Step [200/554], Loss: 0.0752\n",
      "Epoch [110/200], Step [300/554], Loss: 0.0752\n",
      "Epoch [110/200], Step [400/554], Loss: 0.0733\n",
      "Epoch [110/200], Step [500/554], Loss: 0.0748\n",
      "Epoch [110/200], Test Loss: 0.0760\n",
      "Epoch [111/200], Step [100/554], Loss: 0.0753\n",
      "Epoch [111/200], Step [200/554], Loss: 0.0752\n",
      "Epoch [111/200], Step [300/554], Loss: 0.0740\n",
      "Epoch [111/200], Step [400/554], Loss: 0.0738\n",
      "Epoch [111/200], Step [500/554], Loss: 0.0748\n",
      "Epoch [111/200], Test Loss: 0.0760\n",
      "Epoch [112/200], Step [100/554], Loss: 0.0726\n",
      "Epoch [112/200], Step [200/554], Loss: 0.0751\n",
      "Epoch [112/200], Step [300/554], Loss: 0.0761\n",
      "Epoch [112/200], Step [400/554], Loss: 0.0733\n",
      "Epoch [112/200], Step [500/554], Loss: 0.0750\n",
      "Epoch [112/200], Test Loss: 0.0760\n",
      "Epoch [113/200], Step [100/554], Loss: 0.0748\n",
      "Epoch [113/200], Step [200/554], Loss: 0.0753\n",
      "Epoch [113/200], Step [300/554], Loss: 0.0753\n",
      "Epoch [113/200], Step [400/554], Loss: 0.0754\n",
      "Epoch [113/200], Step [500/554], Loss: 0.0737\n",
      "Epoch [113/200], Test Loss: 0.0760\n",
      "Epoch [114/200], Step [100/554], Loss: 0.0752\n",
      "Epoch [114/200], Step [200/554], Loss: 0.0749\n",
      "Epoch [114/200], Step [300/554], Loss: 0.0734\n",
      "Epoch [114/200], Step [400/554], Loss: 0.0755\n",
      "Epoch [114/200], Step [500/554], Loss: 0.0743\n",
      "Epoch [114/200], Test Loss: 0.0760\n",
      "Epoch [115/200], Step [100/554], Loss: 0.0750\n",
      "Epoch [115/200], Step [200/554], Loss: 0.0786\n",
      "Epoch [115/200], Step [300/554], Loss: 0.0745\n",
      "Epoch [115/200], Step [400/554], Loss: 0.0761\n",
      "Epoch [115/200], Step [500/554], Loss: 0.0747\n",
      "Epoch [115/200], Test Loss: 0.0759\n",
      "Epoch [116/200], Step [100/554], Loss: 0.0718\n",
      "Epoch [116/200], Step [200/554], Loss: 0.0739\n",
      "Epoch [116/200], Step [300/554], Loss: 0.0745\n",
      "Epoch [116/200], Step [400/554], Loss: 0.0746\n",
      "Epoch [116/200], Step [500/554], Loss: 0.0745\n",
      "Epoch [116/200], Test Loss: 0.0758\n",
      "Epoch [117/200], Step [100/554], Loss: 0.0724\n",
      "Epoch [117/200], Step [200/554], Loss: 0.0747\n",
      "Epoch [117/200], Step [300/554], Loss: 0.0746\n",
      "Epoch [117/200], Step [400/554], Loss: 0.0734\n",
      "Epoch [117/200], Step [500/554], Loss: 0.0739\n",
      "Epoch [117/200], Test Loss: 0.0758\n",
      "Epoch [118/200], Step [100/554], Loss: 0.0751\n",
      "Epoch [118/200], Step [200/554], Loss: 0.0755\n",
      "Epoch [118/200], Step [300/554], Loss: 0.0765\n",
      "Epoch [118/200], Step [400/554], Loss: 0.0745\n",
      "Epoch [118/200], Step [500/554], Loss: 0.0754\n",
      "Epoch [118/200], Test Loss: 0.0759\n",
      "Epoch [119/200], Step [100/554], Loss: 0.0720\n",
      "Epoch [119/200], Step [200/554], Loss: 0.0752\n",
      "Epoch [119/200], Step [300/554], Loss: 0.0718\n",
      "Epoch [119/200], Step [400/554], Loss: 0.0740\n",
      "Epoch [119/200], Step [500/554], Loss: 0.0757\n",
      "Epoch [119/200], Test Loss: 0.0759\n",
      "Epoch [120/200], Step [100/554], Loss: 0.0746\n",
      "Epoch [120/200], Step [200/554], Loss: 0.0726\n",
      "Epoch [120/200], Step [300/554], Loss: 0.0744\n",
      "Epoch [120/200], Step [400/554], Loss: 0.0743\n",
      "Epoch [120/200], Step [500/554], Loss: 0.0738\n",
      "Epoch [120/200], Test Loss: 0.0758\n",
      "Epoch [121/200], Step [100/554], Loss: 0.0742\n",
      "Epoch [121/200], Step [200/554], Loss: 0.0740\n",
      "Epoch [121/200], Step [300/554], Loss: 0.0752\n",
      "Epoch [121/200], Step [400/554], Loss: 0.0757\n",
      "Epoch [121/200], Step [500/554], Loss: 0.0769\n",
      "Epoch [121/200], Test Loss: 0.0758\n",
      "Epoch [122/200], Step [100/554], Loss: 0.0731\n",
      "Epoch [122/200], Step [200/554], Loss: 0.0758\n",
      "Epoch [122/200], Step [300/554], Loss: 0.0745\n",
      "Epoch [122/200], Step [400/554], Loss: 0.0748\n",
      "Epoch [122/200], Step [500/554], Loss: 0.0721\n",
      "Epoch [122/200], Test Loss: 0.0758\n",
      "Epoch [123/200], Step [100/554], Loss: 0.0754\n",
      "Epoch [123/200], Step [200/554], Loss: 0.0738\n",
      "Epoch [123/200], Step [300/554], Loss: 0.0750\n",
      "Epoch [123/200], Step [400/554], Loss: 0.0754\n",
      "Epoch [123/200], Step [500/554], Loss: 0.0738\n",
      "Epoch [123/200], Test Loss: 0.0756\n",
      "Epoch [124/200], Step [100/554], Loss: 0.0740\n",
      "Epoch [124/200], Step [200/554], Loss: 0.0739\n",
      "Epoch [124/200], Step [300/554], Loss: 0.0748\n",
      "Epoch [124/200], Step [400/554], Loss: 0.0742\n",
      "Epoch [124/200], Step [500/554], Loss: 0.0760\n",
      "Epoch [124/200], Test Loss: 0.0756\n",
      "Epoch [125/200], Step [100/554], Loss: 0.0761\n",
      "Epoch [125/200], Step [200/554], Loss: 0.0728\n",
      "Epoch [125/200], Step [300/554], Loss: 0.0733\n",
      "Epoch [125/200], Step [400/554], Loss: 0.0743\n",
      "Epoch [125/200], Step [500/554], Loss: 0.0751\n",
      "Epoch [125/200], Test Loss: 0.0755\n",
      "Epoch [126/200], Step [100/554], Loss: 0.0743\n",
      "Epoch [126/200], Step [200/554], Loss: 0.0735\n",
      "Epoch [126/200], Step [300/554], Loss: 0.0740\n",
      "Epoch [126/200], Step [400/554], Loss: 0.0721\n",
      "Epoch [126/200], Step [500/554], Loss: 0.0726\n",
      "Epoch [126/200], Test Loss: 0.0755\n",
      "Epoch [127/200], Step [100/554], Loss: 0.0742\n",
      "Epoch [127/200], Step [200/554], Loss: 0.0728\n",
      "Epoch [127/200], Step [300/554], Loss: 0.0739\n",
      "Epoch [127/200], Step [400/554], Loss: 0.0749\n",
      "Epoch [127/200], Step [500/554], Loss: 0.0721\n",
      "Epoch [127/200], Test Loss: 0.0756\n",
      "Epoch [128/200], Step [100/554], Loss: 0.0755\n",
      "Epoch [128/200], Step [200/554], Loss: 0.0742\n",
      "Epoch [128/200], Step [300/554], Loss: 0.0739\n",
      "Epoch [128/200], Step [400/554], Loss: 0.0753\n",
      "Epoch [128/200], Step [500/554], Loss: 0.0724\n",
      "Epoch [128/200], Test Loss: 0.0755\n",
      "Epoch [129/200], Step [100/554], Loss: 0.0742\n",
      "Epoch [129/200], Step [200/554], Loss: 0.0727\n",
      "Epoch [129/200], Step [300/554], Loss: 0.0727\n",
      "Epoch [129/200], Step [400/554], Loss: 0.0758\n",
      "Epoch [129/200], Step [500/554], Loss: 0.0727\n",
      "Epoch [129/200], Test Loss: 0.0754\n",
      "Epoch [130/200], Step [100/554], Loss: 0.0752\n",
      "Epoch [130/200], Step [200/554], Loss: 0.0733\n",
      "Epoch [130/200], Step [300/554], Loss: 0.0734\n",
      "Epoch [130/200], Step [400/554], Loss: 0.0737\n",
      "Epoch [130/200], Step [500/554], Loss: 0.0747\n",
      "Epoch [130/200], Test Loss: 0.0753\n",
      "Epoch [131/200], Step [100/554], Loss: 0.0710\n",
      "Epoch [131/200], Step [200/554], Loss: 0.0748\n",
      "Epoch [131/200], Step [300/554], Loss: 0.0735\n",
      "Epoch [131/200], Step [400/554], Loss: 0.0739\n",
      "Epoch [131/200], Step [500/554], Loss: 0.0753\n",
      "Epoch [131/200], Test Loss: 0.0753\n",
      "Epoch [132/200], Step [100/554], Loss: 0.0738\n",
      "Epoch [132/200], Step [200/554], Loss: 0.0733\n",
      "Epoch [132/200], Step [300/554], Loss: 0.0738\n",
      "Epoch [132/200], Step [400/554], Loss: 0.0748\n",
      "Epoch [132/200], Step [500/554], Loss: 0.0740\n",
      "Epoch [132/200], Test Loss: 0.0753\n",
      "Epoch [133/200], Step [100/554], Loss: 0.0719\n",
      "Epoch [133/200], Step [200/554], Loss: 0.0736\n",
      "Epoch [133/200], Step [300/554], Loss: 0.0734\n",
      "Epoch [133/200], Step [400/554], Loss: 0.0742\n",
      "Epoch [133/200], Step [500/554], Loss: 0.0735\n",
      "Epoch [133/200], Test Loss: 0.0753\n",
      "Epoch [134/200], Step [100/554], Loss: 0.0730\n",
      "Epoch [134/200], Step [200/554], Loss: 0.0734\n",
      "Epoch [134/200], Step [300/554], Loss: 0.0734\n",
      "Epoch [134/200], Step [400/554], Loss: 0.0739\n",
      "Epoch [134/200], Step [500/554], Loss: 0.0726\n",
      "Epoch [134/200], Test Loss: 0.0751\n",
      "Epoch [135/200], Step [100/554], Loss: 0.0727\n",
      "Epoch [135/200], Step [200/554], Loss: 0.0728\n",
      "Epoch [135/200], Step [300/554], Loss: 0.0744\n",
      "Epoch [135/200], Step [400/554], Loss: 0.0762\n",
      "Epoch [135/200], Step [500/554], Loss: 0.0758\n",
      "Epoch [135/200], Test Loss: 0.0752\n",
      "Epoch [136/200], Step [100/554], Loss: 0.0719\n",
      "Epoch [136/200], Step [200/554], Loss: 0.0758\n",
      "Epoch [136/200], Step [300/554], Loss: 0.0739\n",
      "Epoch [136/200], Step [400/554], Loss: 0.0743\n",
      "Epoch [136/200], Step [500/554], Loss: 0.0756\n",
      "Epoch [136/200], Test Loss: 0.0749\n",
      "Epoch [137/200], Step [100/554], Loss: 0.0740\n",
      "Epoch [137/200], Step [200/554], Loss: 0.0748\n",
      "Epoch [137/200], Step [300/554], Loss: 0.0737\n",
      "Epoch [137/200], Step [400/554], Loss: 0.0731\n",
      "Epoch [137/200], Step [500/554], Loss: 0.0729\n",
      "Epoch [137/200], Test Loss: 0.0750\n",
      "Epoch [138/200], Step [100/554], Loss: 0.0738\n",
      "Epoch [138/200], Step [200/554], Loss: 0.0737\n",
      "Epoch [138/200], Step [300/554], Loss: 0.0729\n",
      "Epoch [138/200], Step [400/554], Loss: 0.0729\n",
      "Epoch [138/200], Step [500/554], Loss: 0.0745\n",
      "Epoch [138/200], Test Loss: 0.0750\n",
      "Epoch [139/200], Step [100/554], Loss: 0.0733\n",
      "Epoch [139/200], Step [200/554], Loss: 0.0743\n",
      "Epoch [139/200], Step [300/554], Loss: 0.0747\n",
      "Epoch [139/200], Step [400/554], Loss: 0.0720\n",
      "Epoch [139/200], Step [500/554], Loss: 0.0753\n",
      "Epoch [139/200], Test Loss: 0.0750\n",
      "Epoch [140/200], Step [100/554], Loss: 0.0738\n",
      "Epoch [140/200], Step [200/554], Loss: 0.0746\n",
      "Epoch [140/200], Step [300/554], Loss: 0.0734\n",
      "Epoch [140/200], Step [400/554], Loss: 0.0732\n",
      "Epoch [140/200], Step [500/554], Loss: 0.0741\n",
      "Epoch [140/200], Test Loss: 0.0749\n",
      "Epoch [141/200], Step [100/554], Loss: 0.0729\n",
      "Epoch [141/200], Step [200/554], Loss: 0.0745\n",
      "Epoch [141/200], Step [300/554], Loss: 0.0730\n",
      "Epoch [141/200], Step [400/554], Loss: 0.0711\n",
      "Epoch [141/200], Step [500/554], Loss: 0.0724\n",
      "Epoch [141/200], Test Loss: 0.0749\n",
      "Epoch [142/200], Step [100/554], Loss: 0.0729\n",
      "Epoch [142/200], Step [200/554], Loss: 0.0732\n",
      "Epoch [142/200], Step [300/554], Loss: 0.0726\n",
      "Epoch [142/200], Step [400/554], Loss: 0.0725\n",
      "Epoch [142/200], Step [500/554], Loss: 0.0734\n",
      "Epoch [142/200], Test Loss: 0.0748\n",
      "Epoch [143/200], Step [100/554], Loss: 0.0761\n",
      "Epoch [143/200], Step [200/554], Loss: 0.0741\n",
      "Epoch [143/200], Step [300/554], Loss: 0.0739\n",
      "Epoch [143/200], Step [400/554], Loss: 0.0732\n",
      "Epoch [143/200], Step [500/554], Loss: 0.0737\n",
      "Epoch [143/200], Test Loss: 0.0747\n",
      "Epoch [144/200], Step [100/554], Loss: 0.0730\n",
      "Epoch [144/200], Step [200/554], Loss: 0.0727\n",
      "Epoch [144/200], Step [300/554], Loss: 0.0724\n",
      "Epoch [144/200], Step [400/554], Loss: 0.0749\n",
      "Epoch [144/200], Step [500/554], Loss: 0.0739\n",
      "Epoch [144/200], Test Loss: 0.0748\n",
      "Epoch [145/200], Step [100/554], Loss: 0.0705\n",
      "Epoch [145/200], Step [200/554], Loss: 0.0736\n",
      "Epoch [145/200], Step [300/554], Loss: 0.0741\n",
      "Epoch [145/200], Step [400/554], Loss: 0.0746\n",
      "Epoch [145/200], Step [500/554], Loss: 0.0714\n",
      "Epoch [145/200], Test Loss: 0.0748\n",
      "Epoch [146/200], Step [100/554], Loss: 0.0748\n",
      "Epoch [146/200], Step [200/554], Loss: 0.0730\n",
      "Epoch [146/200], Step [300/554], Loss: 0.0734\n",
      "Epoch [146/200], Step [400/554], Loss: 0.0753\n",
      "Epoch [146/200], Step [500/554], Loss: 0.0730\n",
      "Epoch [146/200], Test Loss: 0.0748\n",
      "Epoch [147/200], Step [100/554], Loss: 0.0733\n",
      "Epoch [147/200], Step [200/554], Loss: 0.0702\n",
      "Epoch [147/200], Step [300/554], Loss: 0.0716\n",
      "Epoch [147/200], Step [400/554], Loss: 0.0737\n",
      "Epoch [147/200], Step [500/554], Loss: 0.0738\n",
      "Epoch [147/200], Test Loss: 0.0747\n",
      "Epoch [148/200], Step [100/554], Loss: 0.0731\n",
      "Epoch [148/200], Step [200/554], Loss: 0.0735\n",
      "Epoch [148/200], Step [300/554], Loss: 0.0739\n",
      "Epoch [148/200], Step [400/554], Loss: 0.0751\n",
      "Epoch [148/200], Step [500/554], Loss: 0.0733\n",
      "Epoch [148/200], Test Loss: 0.0748\n",
      "Epoch [149/200], Step [100/554], Loss: 0.0709\n",
      "Epoch [149/200], Step [200/554], Loss: 0.0745\n",
      "Epoch [149/200], Step [300/554], Loss: 0.0724\n",
      "Epoch [149/200], Step [400/554], Loss: 0.0756\n",
      "Epoch [149/200], Step [500/554], Loss: 0.0738\n",
      "Epoch [149/200], Test Loss: 0.0746\n",
      "Epoch [150/200], Step [100/554], Loss: 0.0729\n",
      "Epoch [150/200], Step [200/554], Loss: 0.0720\n",
      "Epoch [150/200], Step [300/554], Loss: 0.0708\n",
      "Epoch [150/200], Step [400/554], Loss: 0.0720\n",
      "Epoch [150/200], Step [500/554], Loss: 0.0732\n",
      "Epoch [150/200], Test Loss: 0.0747\n",
      "Epoch [151/200], Step [100/554], Loss: 0.0709\n",
      "Epoch [151/200], Step [200/554], Loss: 0.0725\n",
      "Epoch [151/200], Step [300/554], Loss: 0.0739\n",
      "Epoch [151/200], Step [400/554], Loss: 0.0740\n",
      "Epoch [151/200], Step [500/554], Loss: 0.0736\n",
      "Epoch [151/200], Test Loss: 0.0746\n",
      "Epoch [152/200], Step [100/554], Loss: 0.0740\n",
      "Epoch [152/200], Step [200/554], Loss: 0.0727\n",
      "Epoch [152/200], Step [300/554], Loss: 0.0732\n",
      "Epoch [152/200], Step [400/554], Loss: 0.0726\n",
      "Epoch [152/200], Step [500/554], Loss: 0.0753\n",
      "Epoch [152/200], Test Loss: 0.0746\n",
      "Epoch [153/200], Step [100/554], Loss: 0.0743\n",
      "Epoch [153/200], Step [200/554], Loss: 0.0726\n",
      "Epoch [153/200], Step [300/554], Loss: 0.0728\n",
      "Epoch [153/200], Step [400/554], Loss: 0.0736\n",
      "Epoch [153/200], Step [500/554], Loss: 0.0738\n",
      "Epoch [153/200], Test Loss: 0.0746\n",
      "Epoch [154/200], Step [100/554], Loss: 0.0731\n",
      "Epoch [154/200], Step [200/554], Loss: 0.0748\n",
      "Epoch [154/200], Step [300/554], Loss: 0.0736\n",
      "Epoch [154/200], Step [400/554], Loss: 0.0739\n",
      "Epoch [154/200], Step [500/554], Loss: 0.0721\n",
      "Epoch [154/200], Test Loss: 0.0746\n",
      "Epoch [155/200], Step [100/554], Loss: 0.0735\n",
      "Epoch [155/200], Step [200/554], Loss: 0.0710\n",
      "Epoch [155/200], Step [300/554], Loss: 0.0727\n",
      "Epoch [155/200], Step [400/554], Loss: 0.0743\n",
      "Epoch [155/200], Step [500/554], Loss: 0.0736\n",
      "Epoch [155/200], Test Loss: 0.0746\n",
      "Epoch [156/200], Step [100/554], Loss: 0.0723\n",
      "Epoch [156/200], Step [200/554], Loss: 0.0721\n",
      "Epoch [156/200], Step [300/554], Loss: 0.0723\n",
      "Epoch [156/200], Step [400/554], Loss: 0.0700\n",
      "Epoch [156/200], Step [500/554], Loss: 0.0755\n",
      "Epoch [156/200], Test Loss: 0.0744\n",
      "Epoch [157/200], Step [100/554], Loss: 0.0730\n",
      "Epoch [157/200], Step [200/554], Loss: 0.0728\n",
      "Epoch [157/200], Step [300/554], Loss: 0.0736\n",
      "Epoch [157/200], Step [400/554], Loss: 0.0721\n",
      "Epoch [157/200], Step [500/554], Loss: 0.0713\n",
      "Epoch [157/200], Test Loss: 0.0744\n",
      "Epoch [158/200], Step [100/554], Loss: 0.0734\n",
      "Epoch [158/200], Step [200/554], Loss: 0.0720\n",
      "Epoch [158/200], Step [300/554], Loss: 0.0747\n",
      "Epoch [158/200], Step [400/554], Loss: 0.0729\n",
      "Epoch [158/200], Step [500/554], Loss: 0.0727\n",
      "Epoch [158/200], Test Loss: 0.0744\n",
      "Epoch [159/200], Step [100/554], Loss: 0.0726\n",
      "Epoch [159/200], Step [200/554], Loss: 0.0740\n",
      "Epoch [159/200], Step [300/554], Loss: 0.0730\n",
      "Epoch [159/200], Step [400/554], Loss: 0.0738\n",
      "Epoch [159/200], Step [500/554], Loss: 0.0747\n",
      "Epoch [159/200], Test Loss: 0.0745\n",
      "Epoch [160/200], Step [100/554], Loss: 0.0731\n",
      "Epoch [160/200], Step [200/554], Loss: 0.0738\n",
      "Epoch [160/200], Step [300/554], Loss: 0.0714\n",
      "Epoch [160/200], Step [400/554], Loss: 0.0735\n",
      "Epoch [160/200], Step [500/554], Loss: 0.0736\n",
      "Epoch [160/200], Test Loss: 0.0744\n",
      "Epoch [161/200], Step [100/554], Loss: 0.0734\n",
      "Epoch [161/200], Step [200/554], Loss: 0.0727\n",
      "Epoch [161/200], Step [300/554], Loss: 0.0728\n",
      "Epoch [161/200], Step [400/554], Loss: 0.0744\n",
      "Epoch [161/200], Step [500/554], Loss: 0.0737\n",
      "Epoch [161/200], Test Loss: 0.0744\n",
      "Epoch [162/200], Step [100/554], Loss: 0.0732\n",
      "Epoch [162/200], Step [200/554], Loss: 0.0735\n",
      "Epoch [162/200], Step [300/554], Loss: 0.0730\n",
      "Epoch [162/200], Step [400/554], Loss: 0.0730\n",
      "Epoch [162/200], Step [500/554], Loss: 0.0755\n",
      "Epoch [162/200], Test Loss: 0.0743\n",
      "Epoch [163/200], Step [100/554], Loss: 0.0736\n",
      "Epoch [163/200], Step [200/554], Loss: 0.0726\n",
      "Epoch [163/200], Step [300/554], Loss: 0.0728\n",
      "Epoch [163/200], Step [400/554], Loss: 0.0736\n",
      "Epoch [163/200], Step [500/554], Loss: 0.0712\n",
      "Epoch [163/200], Test Loss: 0.0743\n",
      "Epoch [164/200], Step [100/554], Loss: 0.0720\n",
      "Epoch [164/200], Step [200/554], Loss: 0.0731\n",
      "Epoch [164/200], Step [300/554], Loss: 0.0707\n",
      "Epoch [164/200], Step [400/554], Loss: 0.0733\n",
      "Epoch [164/200], Step [500/554], Loss: 0.0722\n",
      "Epoch [164/200], Test Loss: 0.0743\n",
      "Epoch [165/200], Step [100/554], Loss: 0.0726\n",
      "Epoch [165/200], Step [200/554], Loss: 0.0727\n",
      "Epoch [165/200], Step [300/554], Loss: 0.0722\n",
      "Epoch [165/200], Step [400/554], Loss: 0.0743\n",
      "Epoch [165/200], Step [500/554], Loss: 0.0720\n",
      "Epoch [165/200], Test Loss: 0.0743\n",
      "Epoch [166/200], Step [100/554], Loss: 0.0720\n",
      "Epoch [166/200], Step [200/554], Loss: 0.0736\n",
      "Epoch [166/200], Step [300/554], Loss: 0.0739\n",
      "Epoch [166/200], Step [400/554], Loss: 0.0728\n",
      "Epoch [166/200], Step [500/554], Loss: 0.0723\n",
      "Epoch [166/200], Test Loss: 0.0742\n",
      "Epoch [167/200], Step [100/554], Loss: 0.0731\n",
      "Epoch [167/200], Step [200/554], Loss: 0.0726\n",
      "Epoch [167/200], Step [300/554], Loss: 0.0710\n",
      "Epoch [167/200], Step [400/554], Loss: 0.0709\n",
      "Epoch [167/200], Step [500/554], Loss: 0.0728\n",
      "Epoch [167/200], Test Loss: 0.0742\n",
      "Epoch [168/200], Step [100/554], Loss: 0.0717\n",
      "Epoch [168/200], Step [200/554], Loss: 0.0725\n",
      "Epoch [168/200], Step [300/554], Loss: 0.0720\n",
      "Epoch [168/200], Step [400/554], Loss: 0.0722\n",
      "Epoch [168/200], Step [500/554], Loss: 0.0723\n",
      "Epoch [168/200], Test Loss: 0.0742\n",
      "Epoch [169/200], Step [100/554], Loss: 0.0714\n",
      "Epoch [169/200], Step [200/554], Loss: 0.0736\n",
      "Epoch [169/200], Step [300/554], Loss: 0.0722\n",
      "Epoch [169/200], Step [400/554], Loss: 0.0713\n",
      "Epoch [169/200], Step [500/554], Loss: 0.0730\n",
      "Epoch [169/200], Test Loss: 0.0742\n",
      "Epoch [170/200], Step [100/554], Loss: 0.0736\n",
      "Epoch [170/200], Step [200/554], Loss: 0.0725\n",
      "Epoch [170/200], Step [300/554], Loss: 0.0733\n",
      "Epoch [170/200], Step [400/554], Loss: 0.0723\n",
      "Epoch [170/200], Step [500/554], Loss: 0.0729\n",
      "Epoch [170/200], Test Loss: 0.0742\n",
      "Epoch [171/200], Step [100/554], Loss: 0.0727\n",
      "Epoch [171/200], Step [200/554], Loss: 0.0719\n",
      "Epoch [171/200], Step [300/554], Loss: 0.0724\n",
      "Epoch [171/200], Step [400/554], Loss: 0.0722\n",
      "Epoch [171/200], Step [500/554], Loss: 0.0707\n",
      "Epoch [171/200], Test Loss: 0.0743\n",
      "Epoch [172/200], Step [100/554], Loss: 0.0762\n",
      "Epoch [172/200], Step [200/554], Loss: 0.0727\n",
      "Epoch [172/200], Step [300/554], Loss: 0.0744\n",
      "Epoch [172/200], Step [400/554], Loss: 0.0742\n",
      "Epoch [172/200], Step [500/554], Loss: 0.0753\n",
      "Epoch [172/200], Test Loss: 0.0741\n",
      "Epoch [173/200], Step [100/554], Loss: 0.0732\n",
      "Epoch [173/200], Step [200/554], Loss: 0.0720\n",
      "Epoch [173/200], Step [300/554], Loss: 0.0718\n",
      "Epoch [173/200], Step [400/554], Loss: 0.0714\n",
      "Epoch [173/200], Step [500/554], Loss: 0.0722\n",
      "Epoch [173/200], Test Loss: 0.0741\n",
      "Epoch [174/200], Step [100/554], Loss: 0.0706\n",
      "Epoch [174/200], Step [200/554], Loss: 0.0711\n",
      "Epoch [174/200], Step [300/554], Loss: 0.0714\n",
      "Epoch [174/200], Step [400/554], Loss: 0.0731\n",
      "Epoch [174/200], Step [500/554], Loss: 0.0725\n",
      "Epoch [174/200], Test Loss: 0.0741\n",
      "Epoch [175/200], Step [100/554], Loss: 0.0715\n",
      "Epoch [175/200], Step [200/554], Loss: 0.0721\n",
      "Epoch [175/200], Step [300/554], Loss: 0.0726\n",
      "Epoch [175/200], Step [400/554], Loss: 0.0724\n",
      "Epoch [175/200], Step [500/554], Loss: 0.0717\n",
      "Epoch [175/200], Test Loss: 0.0741\n",
      "Epoch [176/200], Step [100/554], Loss: 0.0720\n",
      "Epoch [176/200], Step [200/554], Loss: 0.0733\n",
      "Epoch [176/200], Step [300/554], Loss: 0.0729\n",
      "Epoch [176/200], Step [400/554], Loss: 0.0715\n",
      "Epoch [176/200], Step [500/554], Loss: 0.0742\n",
      "Epoch [176/200], Test Loss: 0.0741\n",
      "Epoch [177/200], Step [100/554], Loss: 0.0739\n",
      "Epoch [177/200], Step [200/554], Loss: 0.0731\n",
      "Epoch [177/200], Step [300/554], Loss: 0.0748\n",
      "Epoch [177/200], Step [400/554], Loss: 0.0732\n",
      "Epoch [177/200], Step [500/554], Loss: 0.0734\n",
      "Epoch [177/200], Test Loss: 0.0740\n",
      "Epoch [178/200], Step [100/554], Loss: 0.0716\n",
      "Epoch [178/200], Step [200/554], Loss: 0.0717\n",
      "Epoch [178/200], Step [300/554], Loss: 0.0725\n",
      "Epoch [178/200], Step [400/554], Loss: 0.0730\n",
      "Epoch [178/200], Step [500/554], Loss: 0.0749\n",
      "Epoch [178/200], Test Loss: 0.0740\n",
      "Epoch [179/200], Step [100/554], Loss: 0.0728\n",
      "Epoch [179/200], Step [200/554], Loss: 0.0737\n",
      "Epoch [179/200], Step [300/554], Loss: 0.0724\n",
      "Epoch [179/200], Step [400/554], Loss: 0.0740\n",
      "Epoch [179/200], Step [500/554], Loss: 0.0725\n",
      "Epoch [179/200], Test Loss: 0.0740\n",
      "Epoch [180/200], Step [100/554], Loss: 0.0715\n",
      "Epoch [180/200], Step [200/554], Loss: 0.0719\n",
      "Epoch [180/200], Step [300/554], Loss: 0.0724\n",
      "Epoch [180/200], Step [400/554], Loss: 0.0713\n",
      "Epoch [180/200], Step [500/554], Loss: 0.0728\n",
      "Epoch [180/200], Test Loss: 0.0741\n",
      "Epoch [181/200], Step [100/554], Loss: 0.0727\n",
      "Epoch [181/200], Step [200/554], Loss: 0.0719\n",
      "Epoch [181/200], Step [300/554], Loss: 0.0714\n",
      "Epoch [181/200], Step [400/554], Loss: 0.0733\n",
      "Epoch [181/200], Step [500/554], Loss: 0.0731\n",
      "Epoch [181/200], Test Loss: 0.0740\n",
      "Epoch [182/200], Step [100/554], Loss: 0.0720\n",
      "Epoch [182/200], Step [200/554], Loss: 0.0725\n",
      "Epoch [182/200], Step [300/554], Loss: 0.0717\n",
      "Epoch [182/200], Step [400/554], Loss: 0.0732\n",
      "Epoch [182/200], Step [500/554], Loss: 0.0724\n",
      "Epoch [182/200], Test Loss: 0.0740\n",
      "Epoch [183/200], Step [100/554], Loss: 0.0716\n",
      "Epoch [183/200], Step [200/554], Loss: 0.0712\n",
      "Epoch [183/200], Step [300/554], Loss: 0.0735\n",
      "Epoch [183/200], Step [400/554], Loss: 0.0729\n",
      "Epoch [183/200], Step [500/554], Loss: 0.0722\n",
      "Epoch [183/200], Test Loss: 0.0740\n",
      "Epoch [184/200], Step [100/554], Loss: 0.0725\n",
      "Epoch [184/200], Step [200/554], Loss: 0.0717\n",
      "Epoch [184/200], Step [300/554], Loss: 0.0723\n",
      "Epoch [184/200], Step [400/554], Loss: 0.0730\n",
      "Epoch [184/200], Step [500/554], Loss: 0.0733\n",
      "Epoch [184/200], Test Loss: 0.0740\n",
      "Epoch [185/200], Step [100/554], Loss: 0.0717\n",
      "Epoch [185/200], Step [200/554], Loss: 0.0730\n",
      "Epoch [185/200], Step [300/554], Loss: 0.0738\n",
      "Epoch [185/200], Step [400/554], Loss: 0.0729\n",
      "Epoch [185/200], Step [500/554], Loss: 0.0718\n",
      "Epoch [185/200], Test Loss: 0.0740\n",
      "Epoch [186/200], Step [100/554], Loss: 0.0714\n",
      "Epoch [186/200], Step [200/554], Loss: 0.0725\n",
      "Epoch [186/200], Step [300/554], Loss: 0.0727\n",
      "Epoch [186/200], Step [400/554], Loss: 0.0716\n",
      "Epoch [186/200], Step [500/554], Loss: 0.0728\n",
      "Epoch [186/200], Test Loss: 0.0739\n",
      "Epoch [187/200], Step [100/554], Loss: 0.0719\n",
      "Epoch [187/200], Step [200/554], Loss: 0.0723\n",
      "Epoch [187/200], Step [300/554], Loss: 0.0746\n",
      "Epoch [187/200], Step [400/554], Loss: 0.0733\n",
      "Epoch [187/200], Step [500/554], Loss: 0.0720\n",
      "Epoch [187/200], Test Loss: 0.0741\n",
      "Epoch [188/200], Step [100/554], Loss: 0.0718\n",
      "Epoch [188/200], Step [200/554], Loss: 0.0720\n",
      "Epoch [188/200], Step [300/554], Loss: 0.0713\n",
      "Epoch [188/200], Step [400/554], Loss: 0.0696\n",
      "Epoch [188/200], Step [500/554], Loss: 0.0717\n",
      "Epoch [188/200], Test Loss: 0.0740\n",
      "Epoch [189/200], Step [100/554], Loss: 0.0718\n",
      "Epoch [189/200], Step [200/554], Loss: 0.0708\n",
      "Epoch [189/200], Step [300/554], Loss: 0.0718\n",
      "Epoch [189/200], Step [400/554], Loss: 0.0719\n",
      "Epoch [189/200], Step [500/554], Loss: 0.0717\n",
      "Epoch [189/200], Test Loss: 0.0738\n",
      "Epoch [190/200], Step [100/554], Loss: 0.0714\n",
      "Epoch [190/200], Step [200/554], Loss: 0.0725\n",
      "Epoch [190/200], Step [300/554], Loss: 0.0731\n",
      "Epoch [190/200], Step [400/554], Loss: 0.0754\n",
      "Epoch [190/200], Step [500/554], Loss: 0.0737\n",
      "Epoch [190/200], Test Loss: 0.0739\n",
      "Epoch [191/200], Step [100/554], Loss: 0.0722\n",
      "Epoch [191/200], Step [200/554], Loss: 0.0725\n",
      "Epoch [191/200], Step [300/554], Loss: 0.0721\n",
      "Epoch [191/200], Step [400/554], Loss: 0.0717\n",
      "Epoch [191/200], Step [500/554], Loss: 0.0723\n",
      "Epoch [191/200], Test Loss: 0.0738\n",
      "Epoch [192/200], Step [100/554], Loss: 0.0718\n",
      "Epoch [192/200], Step [200/554], Loss: 0.0706\n",
      "Epoch [192/200], Step [300/554], Loss: 0.0717\n",
      "Epoch [192/200], Step [400/554], Loss: 0.0720\n",
      "Epoch [192/200], Step [500/554], Loss: 0.0729\n",
      "Epoch [192/200], Test Loss: 0.0740\n",
      "Epoch [193/200], Step [100/554], Loss: 0.0714\n",
      "Epoch [193/200], Step [200/554], Loss: 0.0737\n",
      "Epoch [193/200], Step [300/554], Loss: 0.0758\n",
      "Epoch [193/200], Step [400/554], Loss: 0.0718\n",
      "Epoch [193/200], Step [500/554], Loss: 0.0721\n",
      "Epoch [193/200], Test Loss: 0.0738\n",
      "Epoch [194/200], Step [100/554], Loss: 0.0726\n",
      "Epoch [194/200], Step [200/554], Loss: 0.0737\n",
      "Epoch [194/200], Step [300/554], Loss: 0.0726\n",
      "Epoch [194/200], Step [400/554], Loss: 0.0721\n",
      "Epoch [194/200], Step [500/554], Loss: 0.0715\n",
      "Epoch [194/200], Test Loss: 0.0739\n",
      "Epoch [195/200], Step [100/554], Loss: 0.0763\n",
      "Epoch [195/200], Step [200/554], Loss: 0.0732\n",
      "Epoch [195/200], Step [300/554], Loss: 0.0709\n",
      "Epoch [195/200], Step [400/554], Loss: 0.0707\n",
      "Epoch [195/200], Step [500/554], Loss: 0.0710\n",
      "Epoch [195/200], Test Loss: 0.0739\n",
      "Epoch [196/200], Step [100/554], Loss: 0.0737\n",
      "Epoch [196/200], Step [200/554], Loss: 0.0710\n",
      "Epoch [196/200], Step [300/554], Loss: 0.0737\n",
      "Epoch [196/200], Step [400/554], Loss: 0.0703\n",
      "Epoch [196/200], Step [500/554], Loss: 0.0728\n",
      "Epoch [196/200], Test Loss: 0.0738\n",
      "Epoch [197/200], Step [100/554], Loss: 0.0709\n",
      "Epoch [197/200], Step [200/554], Loss: 0.0729\n",
      "Epoch [197/200], Step [300/554], Loss: 0.0722\n",
      "Epoch [197/200], Step [400/554], Loss: 0.0723\n",
      "Epoch [197/200], Step [500/554], Loss: 0.0717\n",
      "Epoch [197/200], Test Loss: 0.0737\n",
      "Epoch [198/200], Step [100/554], Loss: 0.0704\n",
      "Epoch [198/200], Step [200/554], Loss: 0.0716\n",
      "Epoch [198/200], Step [300/554], Loss: 0.0732\n",
      "Epoch [198/200], Step [400/554], Loss: 0.0717\n",
      "Epoch [198/200], Step [500/554], Loss: 0.0724\n",
      "Epoch [198/200], Test Loss: 0.0737\n",
      "Epoch [199/200], Step [100/554], Loss: 0.0717\n",
      "Epoch [199/200], Step [200/554], Loss: 0.0723\n",
      "Epoch [199/200], Step [300/554], Loss: 0.0727\n",
      "Epoch [199/200], Step [400/554], Loss: 0.0726\n",
      "Epoch [199/200], Step [500/554], Loss: 0.0717\n",
      "Epoch [199/200], Test Loss: 0.0738\n",
      "Epoch [200/200], Step [100/554], Loss: 0.0708\n",
      "Epoch [200/200], Step [200/554], Loss: 0.0733\n",
      "Epoch [200/200], Step [300/554], Loss: 0.0708\n",
      "Epoch [200/200], Step [400/554], Loss: 0.0720\n",
      "Epoch [200/200], Step [500/554], Loss: 0.0718\n",
      "Epoch [200/200], Test Loss: 0.0737\n"
     ]
    }
   ],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout_rate):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc4 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
    "        x = F.relu(self.fc2(x))/2 + x/2\n",
    "        x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
    "        x = F.relu(self.fc3(x)/2) + x/2\n",
    "        x = self.fc4(x)\n",
    "        x = F.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model, and optimizer\n",
    "input_size = X_train.shape[1]\n",
    "output_size = Y_train.shape[1]\n",
    "\n",
    "hidden_size = 512\n",
    "dropout_rate = 0.15\n",
    "num_epochs = 200\n",
    "\n",
    "model = SimpleNN(input_size, hidden_size, output_size, dropout_rate).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "\n",
    "# loss_fn = nn.MSELoss()\n",
    "loss_fn = nn.L1Loss()\n",
    "\n",
    "\n",
    "# training\n",
    "\n",
    "train_loss_list = []\n",
    "test_loss_list = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        mask = targets > -.1\n",
    "        loss = loss_fn(outputs[mask], targets[mask])\n",
    "        # loss = -F.cosine_similarity(outputs, targets).mean()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        train_loss_list.append(loss.item())\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    # eval on test set\n",
    "    model.eval()\n",
    "    total_loss = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            mask = targets > -.1\n",
    "            loss = loss_fn(outputs[mask], targets[mask])\n",
    "            total_loss.append(loss.item())\n",
    "    total_loss = np.mean(total_loss)\n",
    "    test_loss_list.append(total_loss)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Test Loss: {total_loss:.4f}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f8e7cba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss saved to:  ./predictions/3_nn_train_loss.npy\n",
      "Test loss saved to:  ./predictions/3_nn_test_loss.npy\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(len(train_loss_list)) / (len(train_loss_list)-1)\n",
    "plt.plot(x, train_loss_list)\n",
    "\n",
    "x = np.arange(len(test_loss_list)) / (len(test_loss_list)-1)\n",
    "plt.plot(x, test_loss_list)\n",
    "\n",
    "# save train and test loss\n",
    "np.save(predictions_folder+'nn_train_loss.npy', np.array(train_loss_list))\n",
    "np.save(predictions_folder+'nn_test_loss.npy', np.array(test_loss_list))\n",
    "print(\"Train loss saved to: \", predictions_folder+'nn_train_loss.npy') \n",
    "print(\"Test loss saved to: \", predictions_folder+'nn_test_loss.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4e8ad2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN predictions saved to:  ./predictions/3_nn.npy\n"
     ]
    }
   ],
   "source": [
    "# predict on the whole dataset\n",
    "Y_pred = np.zeros((X.shape[0], Y.shape[1]))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i in range(0, X.shape[0], batch_size):\n",
    "        inputs = X_1hot[i:i+batch_size]\n",
    "        inputs = torch.tensor(inputs, dtype=torch.float32).to(device)\n",
    "        outputs = model(inputs)\n",
    "        Y_pred[i:i+batch_size] = outputs.cpu().numpy()\n",
    "\n",
    "# save the whole results for later evaluation\n",
    "np.save(predictions_folder+'nn'+'.npy', Y_pred)\n",
    "print(\"NN predictions saved to: \", predictions_folder+'nn'+'.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cf163a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean L1 loss of NN: 0.07397113796379781\n",
      "MSE loss of NN: 0.023918012985044233\n"
     ]
    }
   ],
   "source": [
    "# compute l1 and MSE loss on test set\n",
    "tmp_mask = (Y_test >= -.1)\n",
    "l1_nn = np.abs(Y_pred[test_mask][tmp_mask] - Y_test[tmp_mask]).mean()\n",
    "mse_nn = ((Y_pred[test_mask][tmp_mask] - Y_test[tmp_mask])**2).mean()\n",
    "print(f\"Mean L1 loss of NN: {l1_nn}\")\n",
    "print(f\"MSE loss of NN: {mse_nn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36531bfb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
